\section{Introduction}
\label{Introduction}
Processor scaling and memory technologies have been developed with different
philosophies in mind.  While processor scaling focused on speed improvements,
memory scaling was centred around increasing the storage capacity.  The
difference in each technology's scaling has led to the Memory Wall~\cite{MemWall}
-- the increasing performance gap between processor and memory
speeds. Data prefetching is one important technique that has been developed 
to bridge this widening performance gap. Data prefetching exploits the fact that in most applications,
 memory accesses have a repeating and predictable pattern to speculatively fetch useful data from slower levels
of the cache hierarchy into faster higher levels of cache. %phrasing?

An ideal prefetching scheme would perfectly capture memory access patterns to
predict and request future accesses in a timely manner. Other important decisions faced
by the prefetcher are when to bring in the prefetch block of data, which level
of the cache hierarchy to place the block in, and what block to evict to
accommodate the incoming block. Memory accesses patterns can be as simple as
fetching the next cache line or a complex pattern such as pointer chasing.
Predicting future accesses requires a trade-off between coverage, the percent of predictions that 
prevented a cache miss, and accuracy, the percent of predictions made that were correct. 
%Gino: Just added a short explanation of coverage and accuracy tradeoff, not sure if necessary
A prefetcher may have high coverage by reducing the number of misses by simply requesting a large 
amount of data, however this does not imply accuracy. Even if the prefetcher brought in data accessed in the future,
a high percentage of the data requested could never be accessed meaning the prefetcher had low accuracy, 
resulting in wasted resources as well as cache pollution. Likewise, a prefetcher can have a 
higher accuracy by being completely sure the data requested is used in the future, 
but may not affect performance since it did not have the coverage needed to make an impact. %end addition.
Most prefetchers maintain an internal confidence mechanism by keeping counters
to track events such as cache hits, misses, references to an entry in the prefetcher's
structure, etc. By comparing the confidence indicated by the counters against different 
threshold values, the aggressiveness of the prefetcher can be adjusted, resulting in lower
coverage and higher accuracy.

For a prefetcher to capture highly irregular access patterns, it needs to be
highly aggressive with high coverage. This causes the prefetcher to recommend many
low accurracy prefetches that lead to the cache being polluted with useless data.  
This effect is more pronounced in a multi-core scenario where the last-level cache is a
shared resource~\cite{Friendly}.  Conversely, a conservative prefetcher would
be highly accurate but might not make enough useful prefetches due to its lower coverage.

We propose Perceptron-based Prefetch Filtering (PPF) as an enhancement to
the existing state of the art prefetchers to robustly overcome the coverage
vs. accuracy trade-off.  The idea is to use a state-of-the-art prefetcher as
the base engine. For that we choose Signature Path Prefetcher (SPP) and
modify the design to make it as aggressive as possible.  This modification
enables the base engine to capture complex memory access patterns and go
deeper into the speculation path, increasing coverage without sacrificing accuracy. 
Normally such aggressive prefetching would come at the cost of increased DRAM traffic
and cache pollution. However, prefetch suggestions recommended by the base prefetch
engine are passed through the perceptron based filter to ensure an accurate prediction.
Over time, PPF learns to correlate the prefetch recommendations with various available
features, leading to elimination of useless prefetches. This filtering leads
to overall increased accuracy of the prefetch scheme and reduction in DRAM
traffic and cache pollution.

\vspace{1ex}This paper describes PPF, giving its merits, analysis and
future scope for expansion. The contributions include:

\begin{itemize}

\item An on-line neural model used for hardware data
  prefetching.  Previous work in this area either relied on program
  semantics~\cite{Semantics} or were application specific~\cite{Datacenter}.
% djimenez: removing reference to this arXiv paper. we are not obligated to cite it.
%, or
%  could not yield a significant improvement over the baseline prefetcher
%  used~\cite{BadPerc}.

\item Implementing the perceptron filter in a robust and accurate
  practical prefetching mechanism, yielding a significant performance
  improvement compared to other state of the art prefetchers.
  Moreover, the perceptron learns to adapt itself to shared resource
  constraints, leading to further increased performance in multi-core and
  lower memory capacity environments.

\item A methodology for determining an appropriate set of 
  features for prediction regardless of the underlying prefetcher used.
  More details are explained in Section \ref{Method-Features}.

\end{itemize}

On a single core configuration, PPF increases performance by 6.84\%
compared to the baseline prefetcher, SPP.  On a multi-core system running a
mix of memory intensive subset of the SPEC 2017 traces, PPF saw an improvement
of 11.9\% over SPP for a 4-core system and 10.67\% for an 8-core system.

The paper is organized as follows. Section~\ref{Background} describes
background work and summarizes SPP.  Section~\ref{Arch} given the 
architectural overview of PPF. Section~\ref{Impl} discusses the
implementation of PPF using SPP and the features used.
Section~\ref{Method} gives the evaluation methodology and explores the feature
space for perceptron learning.  Section~\ref{Results} evaluates the
performance of the prefetcher and Section~\ref{Conclusion} concludes the
paper.
