\section{Background Work and Motivation}
\label{Background}

In this section, we compare the existing work done in the domain of
prefetching.  We also discuss interesting memory access patterns seen in the
workloads in SPEC 2017 benchmark.  The second half of this section gives
background for the Signature Path Prefetcher (SPP) architecture.

\subsection{Related Work}
\label{Background-Related}
The idea of prefetching has been around since Instruction Stream
Buffers were proposed by Jouppi~\cite{ISB}. Some of the earliest data
prefetchers developed to try to identify memory access patterns with a
constant stride pattern~\cite{Smith,Baer,Stride}. Major research was
dedicated to stride distance and depth prediction~\cite{Decoupled,Adaptive}. Newer prefetchers aimed at correlating
predictions with the past memory access addresses~\cite{Address_Correlated,AMPM}.

With time, newer and more intricate prefetchers were
introduced~\cite{Wenisch_Temporal_Streaming,Stealth,Feedback_Directed,Coordinated,Bandwidth_Efficient,Pacman,TLB,Linearizing,Sandbox,VLDP,DoL,Domino}.
A whole class of streaming prefetchers developed on the lines of Spatial /
Temporal Memory
Streaming~\cite{Spatial_Pattern,SMS,Temporal_Instruction_Fetch,Off_Chip,STMS,SMS_JILP}.
Control-flow speculation directed hardware prefetching has also been
proposed~\cite{BFetch,MTBFetch}.

Most modern prefetchers aim at identifying complex memory access patterns in a
given application.  Thus, they capture some of the irregular patterns seen in
pointer chasing data-structures.  Offset based prefetchers are a
generalization of the next-line prefetcher.  Some of the prominent prefetchers
exploiting this idea are Sandbox Prefetcher~\cite{Sandbox} and the Best Offset
Prefetcher~\cite{BOP}.  Another class of prefetchers is lookahead Prefetchers.
Such prefetchers try to speculate deep into the application's memory access
path.  These include Path Confidence based Lookahead Prefetching~\cite{SPP}
and Kill the Program Counter~\cite{KPC}.

A related line of work also explores cache replacement -- insertion policies
and dead block prediction, with an eye towards
prefetching~\cite{DB_Pred,Cache_Burst,KPC,Harmony}.  Nori \textit{et.al.}
introduced prefetching with respect to runtime criticality~\cite{CATCH}.

\textit{Machine Learning and Prefetching:} Peled \textit{et.al.} introduce
interesting ideas for on-line Reinforcement Learning and dynamically scaling
the magnitude of feedback given to the baseline prefetcher~\cite{Semantics}.
The biggest challenge here is that the idea relies on compiler support for
getting features to build the context.

Liao \textit{et. al.} focus on prefetching for data center
applications~\cite{Datacenter}. They use offline machine learning algorithms
such as SVMs and logistic regression to do a parametric search for an optimal
prefetcher configuration.

%% NOTE: The below paper is literally the same idea as ours. But poor implementation and no good results. How much to talk about this paper?
% djimenez: let us not cite this paper. it is in arXiv and we are not
% obligated to cite it. as far as ISCA is concerned it does not exist. there is
% a reason it is only in arXiv and not published somewhere: it is crap.

%Finally, the paper on Data Cache Prefetching with Perceptron
%Learning~\cite{BadPerc} talks about the idea of two step prefetching.
%The first step is an existing baseline prefetcher while the second
%step is a perceptron based throttler.  The paper has lots of
%limitations in terms of design and implementation.  That reflects in
%the fact that it did not lead to any significant performance gain over
%even the basic prefetchers like the Stride Prefetcher~\cite{Stride} and
%the Markov Prefetcher~\cite{Markov}.

\subsection{Perceptron Learning in Architecture}
\label{Background-Perceptron}
Perceptron learning for computer architecture design has been around for a
while.  It was first popularized by Jim\'enez \textit{et. al.} for conditional
branch prediction~\cite{Perc_Branch}. The biggest advantage of the perceptron
model is that it can adjust its weights dynamically to increase / decrease the
importance given to each feature - branch histories in this case.  The
structure we use is derived from the model introduced for Perceptron-based
Reuse Prediction~\cite{Perc_Reuse}, refined with many additional features as
Multiperspective Reuse Prediction{Multiperspective}, and originally derived
from Hashed Perceptron branch prediction{HashedPerceptron}.  Here, a hash of
the feature itself is used to index into a table of perceptron weights.

% djimenez: this detail is unnecessary and likely to invite questions.
% reviewers might ask "what about linear separability?" but the proof is in the
% pudding.
%This form of indexing makes sure that the hyper-plane learned by the
%perceptron weights is able to differentiate between linearly
%inseparable outcomes. 

Perceptron Learning for Reuse Prediction~\cite{Perc_Reuse} extended the hashed
perceptron architecture to perform prediction in context of cache replacement.
The perceptron layer learns to correlate cache replacement behaviour with a
variety of features. The features introduced in that paper were derived from
program counters of the current and the last few memory access instructions
and from the current memory access address.  This led to a prediction
mechanism that is highly accurate, has quick learning rate and is adaptable to
changing program behaviour.

The work was further extended by Multiperspective Reuse
Prediction~\cite{Multiperspective}, which introduced a varied set of
parametric features.  Although design space exploration for feature and
parameter selection was a non-trivial task, once fixed, it yields
replacement predictor highly suitable for the given set of applications.

\textit{Deviations From Actual Perceptrons:} Traditionally, a perceptron
prediction involves multiplying the vector of input features:
F\textsubscript{1xN}, with the corresponding weight vector:
W\textsubscript{Nx1} in a dot product to obtain the sum y\textsubscript{out}.
Here what we use is a perceptron-like structure.  The feature is used to hash
into the weight of perceptrons and the retrieved weights are added straight
away.  Thus, the perceptron algorithm doesn't introduce any multiplication
operations in the inference or training process.  Hence what we adapt in this
work is a perceptron-like learning algorithm as it involves the same principle
involved in perceptron inferencing and training.

\textit{Perceptron Update Rule:} In all the above implementations, a uniform
perceptron update principle is followed.  The weights need to be updated if
the prediction was wrong or the magnituded of the dot product does not exceed
a certain threshold.  After a certain training period, these weights are
proportional to the probability of the outcome being true.  This training
threshold makes sure that the perceptrons are trained until a certain level of
confidence and yet they are not over-trained to the given set.  Care is taken
that the weights saturate at the maximum positive and negative values so that
they remain confined to the bit-width.  The same rules will be applied in the
discussion done in Section \ref{Design-Perceptron} on PPF learning algorithm.

\subsection{Prefetching with SPEC 2017} \label{Background-SPEC2017}
% djimenez: we might not need to discuss the benchmarks too much as they are
% standard. we don't have a lot of room anyway.

\textit{<Discuss SPEC 2017 traces here> <What are the possible graphs /
analysis tools we can use for SPEC traces?>}

\subsection{Baseline Prefetcher: The Signature Path Prefetcher}
\label{Background-SPP}

While the basic idea of perceptron based prefetch filtering is applicable to
any look-ahead prefetcher, we developed a practical implementation of the idea
using SPP~\cite{SPP} as the underlying prefetcher.  SPP is a simple,
lightweight, lookahead prefetcher which can be easily modified to be highly
aggressive.  Here we describe the basic architecture of SPP.

% djimenez: Paul, is all this stuff accurate? SPP continues to confuse me.

\textbf{Signature Table:} The ST keeps track of 256 most recently accessed
pages.  It is meant to capture memory access patterns within a page
boundary.  SPP indexes into an entry of ST using the page number.  For
each entry corresponding to a page, ST stores the `last block offset'
and `old signature'.  Last block offset is the block offset of the
last memory access of that given page.  The block offset is calculated
with respect to the page boundary.  The signature is a 12-bit
compressed representation of the past few memory accesses for that
page.  The signature is calculated as:
$$New Signature = (\,Old Signature << 3 bits\,) \;\;XOR\;\; (\,Delta\,)$$ 
Delta is the numerical difference between the block offset of the 
current and the previous memory acces. In case a matching page entry 
is found, the stored signature retrieved and used to index into the 
Pattern Table.

\textbf{Pattern Table:} The PT is indexed by the signature generated
from ST.  PT holds predicted delta patterns and their confidence
estimates.  Each entry indexed by the signature holds up to 4 unique
delta predictions.  This is implemented by making PT as a 4-way
associative table.

% djimenez: it does look like we're pushing it with all the stuff about SPP.
% Let's describe it enough to give the reader the idea that he/she has a vague
% understanding of how it works and can refer to the MICRO paper for more
% details. (I'm not saying cut anything that's already there, just don't expand.)

%% NOTE: Can we do away with this example? 
%% If it looks like we are writing too much about SPP
%% EDIT: Decided to reorganize and remove this for now.
%% 	Consider the case that incoming page number 10 with a block offset 3
%% 	finds a match in the ST.  The retrieved pattern signature is 0x30 and
%% 	the Last Offset is 1.  Since now there is the Last Block Offset (1),
%% 	Incoming Block Offset (3), Old Signature (0x30), and New Signature
%% 	calculated as per above equation (0x182), SPP can infer
%% 	non-speculatively that the given pattern of memory accesses (as
%% 	captured in the Old Signature) leads to the particular Delta.  In
%% 	general, Delta is defined as the difference between the prefetch
%% 	suggestion block and the initial block which triggered the prefetch.
%% 	In this case, since SPP is in learning phase, it is defined as the
%% 	difference between the Incoming Block Offset and the Last Block Offset
%% 	(+2 in this case).  This, in turn generates the new memory pattern
%% 	(New Signature).  This newly learned signature pattern and the delta
%% 	is stored in the Pattern Table.

\textbf{Prefetch Filter:} The PF is a 1024 entry 1-way associative table that
keeps a record of last few entries prefetched.  This proves useful for
updating the prefetcher states if a tracked prefetch leads to a demand hit or
a cache eviction.

\textbf{Lookahead Prefetching:} On each trigger, SPP tries to go down
the program speculation path using its own prefetch suggestion.
Using current prefetch as the baseline, it re-accesses the PT to generate further
prefetches.  It keeps on repeating this cycle of accessing PT and
updating the signature based on highest confidence prefetch from the
last iteration.  These iteration count till which SPP manages to
predict prefetch entries in the lookahead manner is characterized as
its `depth'.  While doing this, SPP also keeps compounding the
confidence in each depth.  Thus as depth increases, overall confidence
keeps decreasing.  

\textbf{Confidence Tracking}: The PT keeps a track of hits to each
signature through a counter C\textsubscript{sig}.  The number of hits
for a given delta per signature are tracked using a counter
C\textsubscript{delta}.  The confidence for a given delta is
approximated through C\textsubscript{d} = C\textsubscript{delta} /
C\textsubscript{sig}.  When SPP enters into a lookahead mode, the path
confidence P\textsubscript{d} is given as:
$$P\textsubscript{d} \;=\; \alpha  \;.\;  C\textsubscript{d}  \;.\;  P\textsubscript{d-1}$$ 
Here $\alpha$ represents the global accuracy, calculated as the ratio of 
the number of prefetches which led to a demand hit to the number of 
prefetches recommended in total. The range of $\alpha$ is [0,1].

`D' is the lookahead depth. For $d = 1$, when SPP is in
non-speculative mode, P\textsubscript{0} can be thought of as 1. 
The final P\textsubscript{d} is thresholded against prefetching-threshold
(T\textsubscript{p}) to decide whether to prefetch or not.  If yes,
then P\textsubscript{d} is thresholded against a numerically bigger
fill-threshold (T\textsubscript{f}) to decide whether the prefetch
should be sent to L2 Cache (high confidence prefetch) or Last Level
Cache (low confidence prefetch)

<Insert a comprehensive figure about SPP structures>

<Insert another detailed picture about SPP data-path flow>

\subsection{Case for an On-line Throttler}
\label{Background-Case}
As compared to some of the other state of the art prefetchers [BOP], SPP is
less aggressive.  In the single-core environment, this gives BOP an edge as
there is no resource contention among the different cores.  Hence more
aggressive prefetching is bound to prove beneficial.  As we increase the core
count, we observe that SPP starts outperforming rest of the prefetchers.  This
can be attributed to the fact that each prefetch suggested by SPP is a
carefully calculated one and that prevents cache pollution.  \textit{<Figure
showing the variation of SPP vs BOP wrt core count>} For 4-core applications,
SPP suggests XX\% fewer prefetches than BOP and yet leads to higher IPC.

The above analysis shows that with a more careful throttling mechanism, any
prefetcher like SPP can be tuned to become much more aggressive, leading to
increased coverage.  The onus of maintaining the accuracy now falls on the
independent throttler.  To test the hypothesis, we tuned down SPP to the
minimum possible threshold, with the effect of increasing prefetch suggestions
made by SPP by XX\%.  \textit{<Comparison of SPP-Unleased with SPP / BOP>}
Obviously this came at a cost of increased DRAM traffic and cache pollution.

Moreover, the on-line confidence mechanism introduced in SPP was very
rudimentary.  It was based on taking a ratio C\textsubscript{d} =
C\textsubscript{delta} / C\textsubscript{sig} as explained previously. This
confidence was used to make the decision of whether to prefetch or not to
prefetch; and which level to prefetch.  While this approximation was shown to
work in the original implementation, we believe that a better form of
generalised on-line decision making was possible.  Hence, it was necessary to
build a robust and adaptable learning mechanism to accept / reject the
prefetch suggestions; and to decide the fill level (L2 Cache vs Last Level
Cache).

To that effect, we introduce an independent on-line perceptron based
throttling mechanism.
