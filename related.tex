\section{Related Work}
\label{related}

\subsection{Perceptrons in Cache Management}

Perceptron-based prediction mechanisms have proven to yield superior accuracy, which represents an opportunity for optimizations in cache management. 
Khan \textit{et. al.} proposed SDBP \cite{sdbp}, in this work, a sampler structure keeps partial tags of sampled sets separate from the cache. Three tables of two-bit saturating counters are accessed using a technique similar to a skewed branch predictor \cite{Piece_Linear}. For each hit to a block in the sampled set, the program counter (PC) of the relevant memory instruction is hashed into the three tables and the corresponding counters are decremented. For each eviction from a sampled set, the counters corresponding to the PC of the last instruction to access the victim block are incremented. For an LLC access, the predictor is consulted by hashing the PC of the memory access instruction into the three tables and taking the sum of the indexed counters. When the sum exceeds some threshold, the accessed block is predicted to be dead. Tags in sampled sets are managed with true LRU and a reduced associativity, but the LLC may be managed by any policy. The paper applies its predictor to replacement and bypass.

Teran \textit{et al.} propose using perceptron learning for reuse prediction driving a bypass and replacement optimization \cite{Perc_Reuse}. In that work, perceptron learning is used to set weights selected by hashes of multiple features consisting of the PC, several recent PCs, and two different shifts of the tag of the referenced block. Each feature is hashed to index its own table of weights. The selected weights are summed and thresholded to make a prediction. When a sampled block is reused or evicted, the corresponding weights are decremented or incremented, respectively, according to the perceptron learning rule. 

The continuation of Perceptron was later introduced in the work, Multiperspective Reuse Prediction \cite{Multiperspective}. Compared to previous work that focuses on one or two features at a time, Multiperspective prediction uses many features, each contributing to the overall prediction. The prediction enables placement, promotion and bypass optimizations that improves performance.  


\subsection{Base Prefetchers}

Two widely used prefetchers are ?Next-n Lines? \cite{nextnlines} and Stride [4], both of which capture regular memory access patterns with very low hardware overhead. The ?Next-n Lines? prefetcher simply queues prefetches for the next n lines after any given miss, under the expectation that the principle of spatial locality will hold and those cache lines after a missed line are likely to be used in the future. The stride prefetcher is slightly more sophisticated; it attempts to identify simple stride reference patterns in programs based upon the past behavior of missing loads. Similar to the ?Next-n Lines? technique, when a given load misses, cache lines ahead of that miss are fetched in the pattern following the previous be- havior in the hope of avoiding future misses. Both prefetch- ers have been widely used due to the simple design and low hardware overhead. However, without further knowledge about temporal locality and application characteristics, these prefetchers cannot do more than detecting and prefetching regular memory access patterns with limited spatial locality.

Somogyi et al. proposed one of the current top- performing, practical, low-overhead prefetchers, the Spatial Memory Streaming (SMS) prefetcher [23]. SMS leverages code-based correlation to take advantage of spatial locality in the applications over larger regions of memory (called spatial regions). It predicts the future access pattern within a spatial region around a miss, based on a history of access patterns initiated by that missing instruction in the past. While the SMS prefetcher is effective, it is indirectly inferring future program control-flow when it speculates on the misses in a spatial region. As a result, the state overheads of this predictor can be higher than the others in this class.

\subsection{Lookahead Prefetchers}

%ANMP


While previous work had proposed dynamically finding the best prefetch offset, they have failed to look into prefetch timeliness. Michaud proposed the Best-Offset prefetcher \cite{BOP}. A prefetch offset is set automatically and dynamically tries to adapt to the application behavior. A good prefetch offset is determined by looking at two recent access to a particularly line that do not happen too recently. The time between both accesses should be greater than the latency for completing a prefetch request. 

\subsection{Aggressive Prefetching}

A low-accuracy aggressive prefetcher can significantly harm performance. In order to minimize the interference from prefetching, Wu \textit{et. al} proposed PACMan ~\cite{pacman}, a prefetch-aware cache management policy. PACMan dedicated a few sets of the LLC to each of the three competing policies that treat demand and prefetch request differently and uses the policy that shows the lowest number of cache misses. Similarly, Seshadri  \textit{et. al.} propose ICP \cite{icp}. ICP is also designed as a comprehensive mechanism to mitigate prefetching interference. ICP simply demotes a prefetch blow to the lowest priority on a demand hit based on the observation that most prefetches are dead after their first hit. To address prefetcher-caused cache pollution, it also uses a variation of EAF [27] to track prefetching accuracy and inserts only an accurate prefetch to the higher priority position in the LRU stack. ICP assumes, however, that all prefetches are inserted only into the LLC, which restricts the maximum benefit of prefetching.
