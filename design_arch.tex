\section{PPF Architecture}
\label{Arch}

In this section, we talk about the the general structure of the 
Perceptron-Based Prefetch Filtering (PPF) for an underlying prefetcher.  
PPF is a generalised prefetch filtering mechanism and can be adapted to 
most prefetchers with minimal modifications. 

\subsection{The Perceptron Filter}
\label{Arch-Perceptron}
The Perceptron Filter is organised as tables of perceptron weights, 
with a separate table for each feature.  
The feature value is used to index into the corresponding table.
This indexing is done using different numbers of bits for each feature.  
At most 12 bits of a feature are used to index into the
weights table.  Certain features require more resolution power
\textit{i.e.}, the full 12 bits of indexing.  On the other hand, some
features requiring lower resolution require as few as 7 bits of
indexing.  The variable indexing was determined by studying the
features and fine-tuned empirically so as to achieve a good accuracy
vs area trade-off.  Exact details are mentioned under ``Area Overheads''
in Section~\ref{Method-Overheads}

A single entry in the table corresponds to a perceptron weight.  Each
weight is a 5 bit counter - saturating at -16 and +15.  
Our proposed design uses nine features, hence there are nine perceptron 
weight tables. As program execution begins, all the weights are initialized to 0.

\textbf{Inference}\newline In the ChampSim simulator, the prefetching method is 
invoked on every L2 Cache demand access.  At that point, the base prefetcher
has the option to do or not do a prefetch -- if it does, then it has a choice
in how many cache lines to prefetch.  The suggested prefetches can be either
in the L2 Cache or the last-level cache.  The prefetcher decides this
placement, and it is usually done based on an internal confidence mechanism.

When the base prefetcher is triggered, it starts suggesting candidates for
prefetching.  All these recommended prefetches have to be tested through the perceptron
filter to qualify for final prefetching.

The perceptron looks at the microarchitectural state, {\em i.e.} the features,
at that instant.  Each feature is hashed to form an index into a table of up to 
4096 entries dedicated for that feature.  
Feature \#1 indexes into a particular entry in table \#1 and so on.

Once all the weights are retrieved, they are summed.  The sum is thresholded
based on the preset threshold PERC\_ THRESHOLD\_LO.  Only the prefetch
candidates with perceptron sum higher that the threshold qualify for
prefetching.  The prefetches that qualify through the perceptron 
stage are recorded in the Prefetch Table.  In PPF, all the metadata that is required
to recreate the features is also stored in the Prefetch Table.  At a later
stage, when the feedback of the current prefetched line is available, the
stored data can be used to train the perceptron.

The perceptron sum is also used to decide L2 Cache vs Last Level Cache
placement of the prefetch cache line.  All prefetch candidates qualified till
this stage are thresholded against PERC\_THRESHOLD\_HI to decide the fill
level. The two thresholds: PERC\_THRESHOLD\_LO and PERC \_THRESHOLD\_HI are
empirically set.

The perceptron sum can also be seen as the sum of the individual contribution
per feature.  The value of each contribution corresponds to the extent of
confidence of the final output with that given feature.  By summing the
individual contributions, the final perceptron sum denotes the overall
confidence for that prefetch suggestion.  By thresholding the perceptron sum
on two different thresholds, we are dividing the confidence scale of the
prefetch suggestion into three bins.  The first bin corresponds to the lowest
confidence leads to prefetch candidate being rejected.  The next bin
corresponds to prefetches with a moderate confidence level.  Such prefetches
are directed to fill the bigger Last Level Cache and potentially not pollute
the more scarce L2 Cache.  The highest confidence prefetches are filled in the
L2 Cache.

In addition to the Prefetch Table mentioned above, PPF also maintains a
``Reject Table.''  The Reject Table is a 1024-entry deep direct-mapped
table.  If a prefetch suggestion is rejected by the perceptron layer, it is
logged into the Reject Table.  The filer is used to train the perceptron to
avoid false negatives \textit{i.e.}, cases where prediction was to reject the
prefetch but the prefetch would have been useful.

% djimenez: if it were previously stated then it's redundant here? do we need
% this statement?

%As previously stated all the metadata describing the program state at the
%instant of prefetching is stored in the Prefetch Table or the Reject Table.
%This information is useful when the perceptron needs to be updated
%subsequently.

\textbf{Training}\newline In the prefetching environment, feedback for a
prefetch is received whenever there is an eviction or a demand access from the
L2 Cache.  This action triggers training of the base prefetcher as well as the
perceptron layer.  Training the perceptrons involves restoring
the metadata that was stored in Prefetch Table or the Reject Table.  This is
done by indexing into the corresponding tables using the cache line address
of the block being used to train. (10-bits indexing + 6-bits tag matching).
Once the state of the program at the time of prefetching is available, it is
used to index into the perceptron weights table.

If a demand access block that triggers the training was tagged as a valid
prefetch in the Prefetch Table, then the earlier prefetch prediction was
correct.  In that case the perceptron weights are incremented by 1 
if the predicted sum does not cross a pre-defined threshold. These training 
thresholds are introduced to limit the degree of update on correct predictions.
Doing this helps avoid overfitting of the perceptron weights to the given program 
behaviour. These thresholds are referred to as POS\_UPDATE\_THRESHOLD ($\theta_p$) and 
NEG\_UPDATE\_THRESHOLD ($\theta_n$),  respectively for the positive and 
negative values of training saturation.

If a cache block
eviction led to training and the corresponding valid entry was found in the
Prefetch Table, then the prediction made by the perceptron was wrong.  The
perceptron should have ideally rejected the prefetch suggestion as a
low-confidence prefetch.  Here the weights are decremented by 1 to reflect the
misprediction. In either case, weights are saturated at -16 or +15.

A secondary training mechanism also kicks in during demand fetches.  Before
the demand access triggers the next set of prefetches, Reject Table is
checked for a valid entry.  A hit means that the corresponding cache line was
initially suggested by the underlying prefetch engine but rejected by the
perceptron filter.  Thus, the perceptron should have been more confident
about that particular prefetch.  Once such a scenario is identified, the state
of the execution at the time of prefetch is retrieved from the Reject Table.
The retrieved data is used to index into the various weights tables of the
perceptron and the corresponding values are updated by +1, saturating between
and -16 and +15.  This update reflects increased confidence for the prediction
corresponding to that prefetch.

This mechanism allows us to solve the classical problem of exploiting a lost
opportunity.  In prior perceptron based implementations (and in general,
prefetching algorithms), there is usually no way of knowing the result of not
prefetching a particular line.  Our two-step PPF architecture allows us to
overcome that issue.

\subsection{Generalizing PPF for any Prefetcher}
\label{Arch-Generalizing}
The above discussion of PPF shows that it is highly modular and can be
adapted to be used over any base prefecher for increased prefetch accuracy.
In general, only two hooks are required between PPF and the
baseline prefetcher. The first is to make sure that all the prefetch candidates of
the prefetcher pass through the perceptron filter and if qualified, the metadata for
perceptron indexing be stored. The second is needed when the feedback of a
prior prefetch is available in form of a subsequent demand hit or cache
eviction. In that case, the stored metadata needs to be retrieved to update
the state of the perceptrons.

In general, PPF can be adapted to a new base prefetcher with only a few modifications.
\begin{itemize}

\item \textbf{Enhancing the Base Prefetcher:} By tuning down any internal thresholds 
to increase its inherent aggressiveness. 

\item \textbf{Inferencing and Storing:} All prefetch recommendations to be
tested using the perceptron inferencing algorithm.  Perceptron output:
\textit{true} and \textit{false} should be saved appropriately, along with all
the metadata required for perceptron indexing.

\item \textbf{Retrieving and Training:} When the feedback is available, the
previously stored metadata can be used to re-index into the perceptron entries
and increment / decrement the weights.

\item \textbf{Feature Selection:} In Section~\ref{Impl-Features}, we show the features 
used for PPF. Six out of the nine features we developed
use information derived directly from program execution, irrespective of the baseline 
prefetcher. Beyond that, the feature set can be expanded to convey any
useful information from the prefetcher to the perceptron layer.  The 
methodology explained in Section~\ref{Method-Features} talks about developing a minimal 
feature set for PPF.
%For example, in our implementation ``Confidence,'' ``Current Signature XOR Delta,''
%and ``Page Address XOR Confidence'' are such features.

\end{itemize}
