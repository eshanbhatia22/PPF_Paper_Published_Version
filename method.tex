
\section{METHODOLOGY}
\subsection{Performance Model}
For testing and comparing SPP-Perc, we used ChampSim\cite{Champsim}
simulator.  ChampSim is an enhanced version of the framework that was
used for the 2nd Data Prefetch Championship (DPC-2).\cite{DPC_2} We
model model a 1-core / 4-core / 8-core Out of Order machine. The
details of the configuration parameters is summarized in Table
\ref{tab:Sim_params}.

\begin{table}[]
    \centering
    \begin{tabular}{|l|p{3.6cm}|c|}
    \hline
    Level & Configuration & Access Latency \\
    \hline
         L1 Cache & Separate I-cache and D-cache, 32 KB, 8-way & 4 cycles\\
         L2 Cache & Private, 256 KB & 8 cycles\\
         LLC & Shared, 2MB / core, 16-way & 20 cycles\\
         DRAM & 4 GB Single Channel for single-core, 8 GB Double Channel for multi-core & N?? cycles\\
    \hline
    \end{tabular}
    \caption{Memory Model Parameters}
    \label{tab:Sim_params}
\end{table}

The block size was fixed 64KB. Prefetching was only triggered at L2
Cache demand accesses but could be sent to L2 Cache or Last Level
Cache.  No L1 Data level prefetching was done.  LRU replacement policy
was used on all levels of cache hierarchies.  Branch prediction was
done using the perceptron branch predictor\cite{Perc_Branch}.  Page
size was kept as 4KB.  Champsim operates all the prefetchers strictly
in the physical address space.

\subsection{Testing Under Additional Memory Constraints}
The default single-core configuration simulates a 2MB LLC and a single
channel DRAM with 12.8GB/s bandwidth.  We extend the simulations to
include memory constraints introduced in DPC-2.  Specifically we look
at the following two variations:
\begin{itemize}
\item \textit{Low Bandwidth DRAM}: Here the DRAM bandwidth is limited
  to 3.2 GB/s
\item \textit{Small LLC}: In this scenario, LLC size is reduced to 512
  KB
\end{itemize}
All the multi-core simulations are only done in the default
configuration.

\subsection{Workloads}
This is the first time that SPEC 2017 benchmark suite\cite{SPEC2017}
has been used to characterize and measure the prefetch performance.
We use all the 20 workloads available in the SPEC 2017 Suite.  Using
SimPoint\cite{SimPoint} methodology, we identified 95 different
program segments of 1 Billion instructions each.

We performed cross-validation of our SPP-Perc model using SPEC
2006\cite{SPEC2006} and CloudSuite\cite{CloudSuite} benchmarks.  For
SPEC 2006, we developed 94 simpoints spread across the 29
applications.  For CloudSuite, we used the traces made available for
the 2nd Cache Replacement Competition (CRC-2)\cite{CRC_2}.  The traces
include 4 multi-core applications with 6 distinct phases per
application.  In total, we used 285 traces representing workloads
across 53 applications.

\textit{Single-core performance:} For single-core simulations, we use
200 Million instructions to warm-up the microarchitectural structures
and next 1 Billion instructions to do detailed simulations and collect
run-time statistics.  We report the performance as speedup over
baseline \textit{i.e.}, no prefetching.  We take the Instruction Per
Cycle (IPC) of the prefetcher and divide it with the IPC of baseline.
The final number reported is the geometric mean of the speed-up
attained on individual traces.

\textit{Multi-core performance:} For multi-application workloads, we
generate 100 completely random mixes and another 100 mixes from the
memory intensive subset of SPEC 2017.  For 4-core workload, 200
Million instructions are used for warm-up and additional 1 Billion
instruction simulated for collecting statistics.  Each CPU keeps
executing its workload till the last CPU completes 1 Billion
instructions after warm-up.  For collecting IPC and other data, only
the first billion instructions are considered as the ``region of
interest''.

Here we report the weighted speedup normalized to baseline
\textit{i.e.}, no prefetching.  For each of the workloads running on a
particular core of the 4-core 8 MB LLC system, we compute
IPC\textsubscript{i}.  We then find the IPC\_isolated\textsubscript{i}
of the same workload running in isolated 1-core 8 MB LLC environment.
Then we calculate the total weighted-IPC for a given workload mix as
$\Sigma$ (IPC\textsubscript{i} / IPC\_isolated\textsubscript{i}).  For
each of the 100 workload-mix, the sum obtained is normalized to the
weighted-IPC calculated similarly for baseline case \textit{i.e.}, no
prefetching, to get the weighted-IPC-speedup.  Finally the geometric
mean of these 100 weighted-IPC-speedup is reported as the effective
speedup obtained by the prefetching scheme.

We repeat the same process for 8-core workloads with the exception
that 20 Million warm-up instructions and 100 Million full instructions
were executed.  This was done so as to keep the simulation run-time
within reasonable limits as a single 8-core mix was taking up to 5
days to simulate 1 Billion instructions.

\subsection{Preferchers Simulated}
We compared SPP-Perc against three of the latest state of the art
prefetchers: Best Offset Prefetcher (BOP), DRAM Aware - Access Map
Pattern Matching (DA-AMPM)\cite{DA_AMPM} and Signature Path Prefecher
(SPP).  BOP was the winner of 2nd Data Prefetching Championship.
DA-AMPM is the enhanced version of AMPM, modified to account for DRAM
row buffer locality.  SPP has been shown to outperform BOP on SPEC
2006 traces.  For each of these, we compare their speedups taking no
prefetching as the baseline.


\subsection{Developing Features for SPP-Perc}
This section describes the intuition and analysis that went behind
finalizing the features.  As noted earlier, we developed a set of 9
features which allow the perceptron throttler to correlate prefetching
decision with the program context.  To study the correlation across
each feature, we study statistically the perceptron weights and try to
interpret their distribution.

\textbf{GLOBAL PEARSON'S CORRELATION}\newline For this experiment, we
create a dump of the state of perceptron weights at the end of all
trace execution.  The weights obtained from running all the SPEC 2017
traces are concatenated together.  Since the weights are collected at
the end of individual trace execution, the perceptron weights have
attained a relatively stable value by now.  Hence, this dump
represents a `snap' of the trained perceptron weights across all the
SPEC 2017 traces.  The intuition here is that the feature with bulk of
the perceptron weights concentrated around 0 or small magnitude
numbers show a weak correlation with the prefetching outcome.  On the
other hand, features with most of the weights saturated around highest
value (+15) show a high positive correlation and the features with
weights close to the lowest value (-16) show a strong negative
correlation.

We plot a histogram for each feature depicting weights distribution
from -16 to +15 and use this histogram to generate the Pearson's
correlation factor for that feature.  In statistics, Pearson's factor
is a numerical measure of the degree of linear correlation between two
variables.  It ranges from -1 to +1.  The magnitude of Pearson's
factor depicts the extent of correlation and the sign depicts whether
it is a positive correlation or a negative correlation.  Values close
to 0 suggest a low correlation or at times, noisy data.  A value of
+1/-1 suggests a perfectly linear positive / negative correlation
respectively.  Figure XX depicts the histogram and the Pearson's
coefficient for two of the features, YY and ZZ.

Figure XX shows all the features used, arranged in the increasing
order of their Pearson's factor.  As can be seen X out of the 9
features provide a moderate to high correlation, with the magnitude of
P-value > 0.4.  The single most important feature, \_\_ helps provide
a correlation to prefetch outcome with a factor of \_\_.

\textbf{PER TRACE CORRELATION} \newline Another important way to look
at the perceptron features is to see how much their contribution
varies across the traces.  Here we give special attention to features
with low P-values in the previous experiment.  Figure XX shows the
variation of P-values three features : \_\_, \_\_ and \_\_; across all
the SPEC 2017 traces.  For simplicity, the traces are arranged in an
increasing order of contribution made by the feature and the trace
names have been omitted.  It can be seen that even features with a low
overall correlation provide useful correlation (magnitude > 0.4) for
XX out of the 95 simpoints developed for SPEC 2017.  

\textbf{TRIMMING FEATURES USING CORRELATION} \newline Besides
providing interesting insights into prefetching behaviour, P-value can
also be used for feature selection and prefetcher tuning.

Here, we introduce the concept of cross-correlation across features.
Just as we examined correlation of each feature with the final
outcome, we can also study correlation between the features.  We used
the above methodology to eliminate features which provide a little or
no information that has already been captured in other features.

As a part of feature selection, we initially came up a mix of 23
features- primary and composite.  By studying cross correlation of
each of these features against others in a 23x23 matrix, we identified
pairs of features with correlation factor > 0.9 in magnitude and
carefully eliminated redundant features.  Using this approach, we
managed to reduce the feature count to 9.  Thus, in the final
implementation of SPP-Perc, no two features have a high correlation
between them.  This way we can be sure that each feature is bringing
in contribution that cannot be captured using other features.

\textit{<TODO: VALIDATE THIS>} Secondly, studying the relative
importance of each feature enabled us to vary the number of entries
dedicated for each feature.  Features with higher Pearson's
correlation (like \_\_ and \_\_) were given most importance and
allowed full 12-bits of indexing.  Features like \_\_ and \_\_, with a
low overall P-value were allocated fewer indexes in the feature table.

To conclude, in the above discussion we justify the features for
perceptron from a statistical viewpoint.  We also show how this
information can also be used for prefetcher tuning.  All this study
was made possible only because we used on-line perceptron learning for
prefetching and that enabled us to examine the weights in detail.


\subsection{Overhead for SPP-Perc}

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|m{4.8cm}|}
    \hline
        \textbf{Field} &
        \textbf{Bits} &
        \textbf{Comment} \\
    \hline
         Valid & 1 & Indicates a valid entry in the filter\\
         Tag & 6 & Identifier for the entry in the filter\\
         Useful & 1 & To show if the given entry led to a useful demand fetch\\
         Perc Decision & 1 & Prefetched vs Not-prefetched \\
    \hline
        PC & 12 & \multirow{5}{4.8cm}{Meta-data required for perceptron training}\\
        Address & 24 & \\
        Delta & 7 & \\
        Curr Signature & 12 & \\
        Confidence & 7 & \\
    \hline
        \multicolumn{3}{|c|}{\textbf{Total 84 bits}}\\
    \hline
    \end{tabular}
    \caption{Meta-data Stored in Prefetch Filter}
    \label{tab:PF_metadata}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{|p{1.5cm}|p{1cm}|c|p{1.5cm}|}
    \hline
        \textbf{Structure} &
        \textbf{Entry} &
        \textbf{Components} &
        \textbf{Total} \\
    \hline
        \multirow{5}{1.5cm}{Signature Table} & \multirow{5}{1.5cm}{256} & Valid (1 bit) & \multirow{5}{1cm}{11008  bits}\\
        & & Tag (16 bits) &\\
        & & Last Offset (6 bits) &\\  
        & & Signature (12 bits) &\\
        & & LRU (6 bits) &\\
    \hline
        \multirow{3}{1.5cm}{Pattern Table} & \multirow{3}{1.5cm}{512} & $C_{sig}$ (4bits) &\multirow{3}{1cm}{24576 bits}\\
        & & $C_{delta}$ (4*4 bits) &\\
        & & Delta (4*7 bits) &\\
    \hline
        \multirow{3}{1.5cm}{Perceptron Tables} & \multirow{3}{1.5cm}{4096*8 128*1} & \multirow{3}{1.5cm}{5 bits} & \multirow{3}{1.5cm}{164480 bits}\\
        & & &\\
        & & &\\
    \hline
        \multirow{2}{1.5cm}{Prefetch Filter \footnotemark[1]} & \multirow{2}{1.5cm}{1024} & \multirow{2}{1.5cm}{84 bits} & \multirow{2}{1.5cm}{86016 bits}\\
        & & &\\
    \hline
     \multirow{2}{1.5cm}{Reject Filter \footnotemark[2]} & \multirow{2}{1.5cm}{1024} & \multirow{2}{1.5cm}{83 bits} & \multirow{2}{1.5cm}{84992 bits}\\
        & & &\\
    \hline
        \multirow{4}{1.5cm}{Global History Register} & \multirow{4}{1.5cm}{8} & Signature (12 bits) & \multirow{4}{1.5cm}{264 bits}\\
        & & Confidence (8 bits) & \\
        & & Last Offset (6 bits) & \\
        & & Delta (7 bits) & \\
    \hline
        \multirow{2}{1.5cm}{Accuracy Counters} & 1 & C$_{total}$ & 10 bits\\
        & 1 & C$_{useful}$ & 10 bits\\
    \hline
        \multirow{3}{1.5cm}{Global PC Trackers} & \multirow{3}{1cm}{3} & $PC_1$ (12 bits)& \multirow{3}{1.cm}{36 bits}\\
        & & $PC_2$ (12 bits) &\\
        & & $PC_3$ (12 bits) &\\
    \hline
        \multicolumn{4}{|c|}{\textbf{Total: 371356 bits = 45.33 KB}}\\
    \hline
    \end{tabular}
    \caption{SPP-Perc Storage Overhead}
    \label{tab:SPPPerc_overhead}
\end{table}
\footnotetext[1]{Components of Prefetch Filter can be found in Table \ref{tab:PF_metadata}}
\footnotetext[2]{RF does not need to maintain the useful bit as that only applies for prefetches that ultimately made through}

In this section, we analyze the hardware overhead required to
implement SPP-Perc.  The Prefetch Filter was enhanced to accommodate
storing of meta-data for perceptron training.  Table
\ref{tab:PF_metadata} depicts the meta-data stored for each entry in
the Prefetch Filter.  Table \ref{tab:SPPPerc_overhead} shows the total
storage overhead of SPP-Perc implementation.  The hardware budget for
2nd Data Prefetching championship was 32 KB.  Keeping that in mind and
the speedup SPP-Perc obtained over the winner, the extra hardware
budget can be accounted for.  The extra hardware also makes the
overall scheme more scalable than SPP.  In the original SPP paper, it
was demonstrated that adding extra hardware brings little advantage in
terms of performance gain.  The newly added perceptron tables can be
scaled to increase / decrease features depending on the permitted
budget.

In terms of computations, the perceptron mechanism only introduces an
extra adder tree.  The hash perceptron mechanism makes sure than there
is no actual vector multiplication happening in the hardware.
Obtaining the perceptron sum requires addition of 9 5-bit numbers.
Using an adder tree of 4 5-bit adders, this can be done in
ceil($log_{2}9$) = 4 steps.  Perceptron update only requires weight
update by +1 or -1.  Thus, all the operations required for perceptron
inferencing or updating the states of the perceptron throttler can be
easily done in the time constraints of L2 Cache Accesses.

