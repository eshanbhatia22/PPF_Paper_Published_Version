%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is the template for submission to ISCA 2018
% The cls file is a modified from  'sig-alternate.cls'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{sig-alternate} 
\usepackage{mathptmx} % This is Times font

\newcommand{\ignore}[1]{}
\usepackage{fancyhdr}
\usepackage[normalem]{ulem}
\usepackage[hyphens]{url}
\usepackage{microtype}
\usepackage{fixltx2e}
\usepackage{array}
\usepackage{multirow}

% Always include hyperref last
\usepackage[bookmarks=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=black,citecolor=blue,urlcolor=black]{hyperref}

% Ensure letter paper
\pdfpagewidth=8.5in
\pdfpageheight=11in


%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\newcommand{\iscasubmissionnumber}{NaN}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\fancypagestyle{firstpage}{
  \fancyhf{}
\renewcommand{\headrulewidth}{0pt}
  \fancyhead[C]{\normalsize{}} 
  \fancyfoot[C]{\thepage}
}  

\pagenumbering{arabic}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{SPP-Perc: Peceptron Learning Based Enhanced Signature Path Prefetcher} 
\author{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\thispagestyle{firstpage}
\pagestyle{plain}

\begin{abstract}
\textbf{Hardware prefetching has been introduced in modern processors as a way to reduce compulsory cache misses. 
An efficient prefetcher should be able to identify complex memory access patterns. 
This enables it to fetch a block ahead of its demand access. 
}

\textbf{In this paper, we introduce perceptron learning to help make this prefetching decision efficiently. 
We have taken a state of the art prefetcher - Signature Path Prefetcher (SPP) as the underlying prefetch engine. 
The perceptron layer acts as a throttler filters out the unnecessary prefetches recommended by SPP. 
We have also explored a range of features that can be used to train the perceptron layer. 
Our results show that SPP-Perc improves performance on the memory intensive subset of the SPEC 2017 Benchmark suite by 6.84\% on single-core and by 11.9\% on multi-core traces, as compared to just SPP.
We also demonstrate that the performance gained from using our efficient throttler scales better with increasing number of cores.
}
\end{abstract}

\section{Introduction}
Since past few decades, processor and memory scaling has been enabled with different philosophies in mind. 
While processor scaling focused on speed improvements, memory scaling was centred around increasing the storage capacity. 
The process technology reaching scaling limitations led to Memory Wall [cite] - the increasing performance gap between the processor speed and the memory speeds. 
Various techniques have been developed to combat this widening gap. 
Data prefetching is one such important technique. 
Data prefetching exploits the fact that most applications, data is accessed with some spatial or temporal locality.

An ideal prefetching scheme involves capturing the memory access patterns to predict future prefetches in a timely manner. 
Other important decisions faced by the prefetcher are when to bring in the prefetch block of data, which level of cache hierarchy to place the block in and finally, what block to evict to accommodate the incoming block. 
Memory accesses patterns can be as simple as fetching the next cache line or a complex but predictable pattern, like in pointer chasing applications. 
Predicting this often requires a trade-off between coverage and accuracy. 
For a prefetcher to capture highly irregular access patterns, it needs to be highly aggressive (high coverage). 
That also means the prefetcher would end up recommending a lot of useless prefetches, leading to cache pollution (low accuracy). 
This effect would get even more pronounced in multi-core scenario where the last level cache is a shared resource. 
Conversely, a conservative prefetcher would be highly accurate but might not make enough useful predictions.

To this effect we propose SPP-Perc <come up with a catchy name>. 
It is an enhancement to the existing state of the art prefetcher and overcomes the coverage vs accuracy trade-off in a robust manner. 
The idea is to modify SPP design to make it as aggressive as it can get. 
This enables SPP to capture complex memory access patterns hence leading to increased coverage.
This comes at a cost of increased DRAM traffic, hence reduction in overall IPC.
The prefetch suggestions recommended by SPP pass through the perceptron based throttler. 
Over time, perceptron layer learns to correlate the prefetches recommended by SPP with various available features and eliminate useless prefetches. 
This leads to an overall increased accuracy of the prefetch scheme and reduction in DRAM traffic.

\vspace{1ex}This paper talks about SPP-Perc, its merits, analysis and future scope for expansion. 
The contributions made by this paper are:

\begin{itemize}
\item This is the first time where a truly on-line learning based neural network model has been used when it comes to hardware data prefetching. 
The prior works in this area either relied on program semantics [cite] or were application specific [cite] or could not yield a significant improvement over the baseline prefetcher used. [cite]

\item Implementing the pereptron throttler in a robust and accurate practical prefetching mechanism. 
The overall scheme yields increased accuracy as compared to the other state of the art prefetchers.
Moreover, the perceptron throttler learns to adapt itself to shared resource constraints - leading to an even increased performance in multi-core and lower capacity / bandwidth environments. 

\item Examining the perceptron weights doing a statistical study to quantify the contribution made by each feature.
We have also used this idea to develop a comprehensive set of features where each feature contributes some 'new' information to the perceptron.
The details are explained under methodology, in Section XX.

\item Cross-validating the design across benchmarks. 
All the development and parameter tuning was done on the SPEC 2017 benchmark. 
The results have been shown to be consistent across unseen benchmarks.
More details can be found in the results section (Section XX).

%\item Designing the prefetcher keeping SPEC 2017 benchmark suite in mind. 
%We have used SPEC 2017 to compare SPP-Perc against the highest performing prefetchers. 
%We have also used our prefetcher to characterize memory intensive benchmarks of the SPEC 2017 suite, with respect to prefetching.

%\item We came up with an efficient model to use the perceptron output as a representation of the prefetch confidence using an on-line histogram technique. 
%We show how the simple sum can replace the existing mechanism which was essentially based on counting the number of hits for a particular entry in SPP.

\end{itemize}

We tested SPP-Perc on Champsim, an open source simulator which models an Out-of-Order CPU using. 
For evaluation, we used SPEC 2017 benchmarks. 
SPEC 2006 and CloudSuite were used for cross-validation of the prefetcher implementation.
On a single core configuration, SPP-Perc increases performance of the system by 39.5\% as compared to the baseline with no prefetching, on the memory intensive subset of the benchmark.
This outperforms the next best prefetcher, SPP,  by 6.84\%. 
On a multi-core system running a mix of memory intensive subset of the SPEC 2017 traces, SPP-Perc saw an improvement 5.95\% over SPP for 4-core system and 10.67\% for 8-core system.

In the Results part in section XX, we show a detailed analysis of the speedup achieved and also talk about speedup achieved on SPEC 2006 and CloudSuite benchmarks.

\vspace{1ex}The rest of the paper is organized as follows:

\begin{itemize}
\item Section 2 talks about background work and recaptures the SPP architecture
\item Section 3 discusses enhancements made to the SPP design and the detailed implementation of the perceptron mechanism
\item Section 4 consists of the methodology for evaluating SPP-Perc and exploring the feature space for perceptron learning
\item Section 5 comprises results and evaluation in terms of IPC, MPKI, coverage and DRAM traffic, for a variety of benchmark suites
\item Section 6 is the conclusion and future scope of expansion on this work
\end{itemize}

\section{Background Work and Motivation}

In this section, we compare the existing work done in the domain of prefetching. 
We also discuss about some of the interesting memory access patterns seen in the workloads in SPEC 2017 benchmark. 
The second half of this section deals with a background of Signature Path Prefetcher architecture.

\subsection{Motivation}
Some of the earliest data prefetchers developed to leverage this locality were stride based [cite:\newline
- A. J. Smith. Sequential Program Prefetching in Memory Hierarchies (1978)\newline
- J.-L. Baer and T.-F. Chen. An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty (1991)\newline
]\newline

Major research in prefetching domain was based on stride length and depth prediction [cite:\newline
- Decoupled Predictor-Directed Stream Prefetching
Architecture (2003)\newline
- Memory Prefetching Using Adaptive Stream Detection (2006)\newline
]\newline

With time newer and more intricate prefetchers were introduced. [Cite:\newline
- Temporal Streaming of Shared Memory (2005)\newline
- Stealth Prefetching (2006)\newline
- Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers (2007)\newline
- Coordinated Control of Multiple Prefetchers in Multi-Core Systems (2009)\newline
- Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems (2009)\newline
- PACMan: PrefetchAware Cache Management for High Performance Caching (2011)\newline
- TLB Improvements for Chip Multiprocessors: Inter-Core Cooperative Prefetchers and Shared Last-Level TLBs (2013)\newline
- Linearizing Irregular Memory Accesses for Improved Correlated Prefetching (2013)\newline
- Sandbox Prefetching: Safe run-time evaluation of aggressive prefetchers\newline
]\newline

Prefetchers often tend to correlate predictions with the past memory access addresses [cite: Making Address-Correlated Prefetching Practical (2010)]. 
 A whole class of streaming prefetchers developed on the lines of Spatial / Temporal Memory Streaming
[ Cite:\newline
- Accurate and Complexity-Effective Spatial Pattern Prediction (2004)\newline
- Spatial Memory Streaming (2006)\newline
- Temporal Instruction Fetch Streaming (2008)\newline
- Practical Off-Chip Meta-Data for Temporal Memory Streaming (2009)\newline
- Spatio-Temporal Memory Streaming (2009)\newline
- Spatial Memory Streaming with Rotated Patterns (2009)\newline
- Spatial Memory Streaming.” Journal of Instruction-Level Parallelism" (2011)\newline
]\newline

More modern prefetchers are often offset based <cite BOP and other offset based> or lookahead based <cite SPP, KPC and a few more>.

A related line of work also explored cache replacement: insertion policies and dead block prediction, with an eye towards prefetching. [Cite:\newline
- Dead-Block Prediction and Dead-Block Correlating Prefetchers (2001)\newline
- Cache Bursts: A New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency (2008)\newline
- KPC (2017)\newline
- Rethinking Belady's Algorithm to Accommodate Prefetching (2018)\newline
]

\subsection{Prefetching with SPEC 2017}
<Discuss SPEC 2017 traces here>
<What are the possible graphs / analysis tools we can use for SPEC traces?>

\subsection{Perceptron Learning in Architecture}

Perceptron learning for computer architecture design has been around for a while. 
It was popularized by [cite initial branch prediction paper]. 
In it's original implementation, the Program Counter of the instruction looking for a branch prediction would index into a given table of perceptron weights. 
The retrieved weights would then be multiplied with the history of branch outcomes stored in the History Register (which is a feature) in a dot product fashion. 
The sum obtained is thresholded at 0 \textit{i.e.}, if the prediction y\textsubscript{out} >= 0, the sum is assumed to indicate a  high confidence taken branch \textit{true}. 
Else the confidence for branch taken is not high enough and the prediction is made as \textit{false}.

The structure that we use is derived from the model introduced in [cite piecewise linear branch prediction]. 
Here the feature itself is used to index into a hash of perceptron weights. 
This form of indexing makes sure that the hyper-plane learned by the perceptron weights is able to differentiate between linearly inseparable outcomes. 
<Is it true? Not sure>

Perceptron Learning for Reuse Prediction [cite] extended the hash perceptron architecture to perform prediction in context of cache replacement. 
Perceptron layer can learn to correlate the cache replacement behaviour with a variety of features. 
Each feature has its dedicated table of perceptron weights which it indexes into. 
Hence number of tables equals the number of features. 
Once the perceptron weights from all the tables are obtained, they're summed and thresholded based on a pre-decided value. 
This led to a prediction mechanism that is highly accurate, has quick learning rate and is adaptable to changing program behaviour. 
The features introduced in that paper were derived from Program Counters of the current and the last few instructions and the memory address of the current access. 
Even with such basic features, high level of correlation could be established by perceptrons. 

The work was further extended by Multiperspective Reuse Prediction[cite], which introduced a varied set of parametric features. 
Although design space exploration for feature and parameter selection was a non-trivial task, once fixed, it could give a cache replacement predictor highly suitable for the given set of applications.
\newline
\newline
\textit{Deviations From Actual Perceptrons:} Traditionally, a perceptron prediction involves multiplying the vector of input features: F\textsubscript{1xN}, with the corresponding weight vector: W\textsubscript{Nx1} in a dot product fashion to obtain the sum y\textsubscript{out}. 
Here what we use is a perceptron-like structure. 
The feature is used to hash into the weight of perceptrons and the retrieved weights are added straight away. 
This can be seen a dot product of $\vec{1}$\textsubscript{1xN} with W\textsubscript{Nx1}. 
This way, the perceptron algorithm doesn't introduce any multiplication operations in the inference or training process. 
Hence what we adapt in this work, is a perceptron-like learning algorithm as it involves the same principle involved in perceptron inferencing and training. 
For simplicity, we will call our implementation as perceptron implementation.
\newline
\newline
\textit{Perceptron Update Rule:} In all the above implementations, a uniform perceptron update principle is being followed. 
The weights need to be updated if the prediction was wrong or the predicted sum does not cross a certain threshold. 
After a certain training period, these weights are proportional to the probability of the outcome being true. 
This training threshold makes sure that the perceptrons are trained till a certain level of confidence and yet they are not over-trained to the given set. 
Care is taken that the weights saturate at certain positive and negative values so that they remain confined to the bit-width. 
The same rules will be applied in the discussion done in section 3.2 on SPP-Perc learning algorithm.

\subsection{Signature Path Prefetcher Architecture}
Since the work we have done builds heavily on SPP[cite], we are presenting a brief recap of the ideas introduced in the original paper, especially with an eye towards their relevance in our work. 
Signature Path Prefetcher (SPP) is a lookahead prefetcher and consists of following structural components:
\newline
\newline
\textbf{Signature Table:} ST keeps track of 256 most recently accessed pages. 
It is meant to capture memory access patterns within a page boundary. 
SPP indexes into an entry of ST using the page number. 
For each entry corresponding to a page, ST stores the 'last block offset' and 'old signature'. 
Last block offset is the block offset of the last memory access of that given page. 
The block offset is calculated with respect to the page boundary. 
The signature is a 12-bit compressed representation of the past few memory accesses for that page. 
The signature is calculated as:
$$New Signature = (\,Old Signature << 3 bits\,) \;\;XOR\;\; (\,Delta\,)$$ 
In case a matching page entry is found, the stored signature in the corresponding entry is retrieved.
\newline
\newline
Consider the case that incoming page number 10 with a block offset 3 finds a match in the ST. 
The retrieved pattern signature is 0x30 and the Last Offset is 1. 
Since now there is the Last Block Offset (1), Incoming Block Offset (3), Old Signature (0x30), and New Signature calculated as per above equation (0x182), SPP can infer non-speculatively that the given pattern of memory accesses (as captured in the Old Signature) leads to the particular Delta. 
In general, Delta is defined as the difference between the prefetch suggestion block and the initial block which triggered the prefetch. 
In this case, since SPP is in learning phase, it is defined as the difference between the Incoming Block Offset and the Last Block Offset (+2 in this case). 
This, in turn generates the new memory pattern (New Signature). 
This newly learned signature pattern and the delta is stored in the Pattern Table.
\newline
\newline
\textbf{Pattern Table:} The PT is indexed by the signature generated from ST. 
PT holds predicted delta patterns and their confidence estimates. 
Each entry indexed by the signature holds up to 4 unique delta predictions. 
This is implemented by making PT as a 4-way associative table. 
To measure an estimate of a delta confidence, PT implements two counters: C\textsubscript{sig} for each entry indexed by the signature and C\textsubscript{delta} for each delta entry for a given signature. 
More details of this on-line score-keeping mechanism is described under Confidence Tracking discussion.

\textit{Lookahead Prefetching:} For each trigger to the prefetcher, the first time that the SPP indexes into the PT, it does so in the non-speculative manner. 
We call it non-speculative because the parameters which SPP uses to predict, like last block offset, incoming block offset and signature are well-defined. 
Assuming PT generates a valid delta prediction (rather, up to 4 valid deltas), SPP enters into the lookahead mode.

In the lookahead mode, SPP looks at the last generated delta(s), chooses the one with the highest confidence and speculatively generates the new effective address and the new confidence. 
Using these new parameters, it re-accesses the PT to generate further prefetches. 
SPP keeps on repeating this cycle of accessing PT and updating the signature based on highest confidence prefetch from the last iteration. 
These number of iterations are characterized by the 'depth' till which SPP manages to predict prefetch entries in the lookahead manner. 
While doing this, SPP also keeps compounding the confidence in each depth. 
Thus as depth increases, overall confidence keeps decreasing. 
This makes sense since we are going deeper into the speculation path. 
When the confidence falls below a pre-defined threshold, SPP stops further lookahead iterations
\newline
\newline
\textbf{Global History Register:} One of the innovative ideas introduced in SPP was the Global History Register, which bootstraps learning across page boundaries.
GHR is a small 8-entry fully associative structure. 
Whenever a prefetch suggestion from the Pattern Table crosses page boundaries, instead of completely discarding it, GHR saves that information for inter-page learning.
Since the SPP-Perc implementation in this paper doesn't affect GHR functionality in any ways, we won't be going into more details about GHR.
\newline
\newline
\textbf{Prefetch Filter:} SPP also introduced the concept of Prefetch Filter - a 1024 entry 1-way associative table which keeps a record of last few entries prefetched by SPP. 
This proves useful for following reasons:
\begin{itemize}
    \item \textbf{Eliminating duplicate prefetches}\newline 
    Any subsequent prefetches recommended by SPP mechanism is checked against the existing entries in Prefetch Filter. 
If a valid entry is found, it means that the particular cache line is already either in L2 Cache or being fetched. 
Hence there is no need to issue a duplicate prefetch.
    
    \item \textbf{Updating prefetcher}\newline
    If a subsequent demand access comes before eviction for a prefetch that was recommended by SPP, various components of SPP are updated to reflect that this was a good prefetch. 
Correspondingly, if a prefetched cache line is evicted without being demand hit even once, SPP learns that this was a useless prefetch and updates the various tables accordingly
    
    \item \textbf{Tracking Global Accuracy}\newline
    PF keeps a track of usefulness of the prefetches by introducing 2 counters: C\textsubscript{total} to track the total number of prefetches getting recorded in the PF and C\textsubscript{useful} to keep a track of which prefetches recorded in the PF led to a demand hit. 
The ratio C\textsubscript{useful} / C\textsubscript{total} is the global accuracy for the prefetching scheme. 
It is represented as $\alpha$ in the paper. 
The global accuracy acts a real-time throttler for the path confidence value as depicted in the following discussion.
\end{itemize}
\textbf{Confidence Tracking}: The PT keeps a track of hits to each signature through a counter C\textsubscript{sig}. 
The number of hits for a given delta per signature are tracked using a counter C\textsubscript{delta}. 
The confidence for a given delta is approximated through C\textsubscript{d} = C\textsubscript{delta} / C\textsubscript{sig}. 
When SPP enters into a lookahead mode, the path confidence P\textsubscript{d} is given as:
$$P\textsubscript{d} \;=\; \alpha  \;.\;  C\textsubscript{d}  \;.\;  P\textsubscript{d-1}$$ where $\alpha$ is called the global accuracy and is calculated as shown in the Prefetch Filter section. 
The range of global accuracy is in [0,1]. 
The lower the value of $\alpha$ for a given trace or a program phase, the overall confidence is scaled down and this makes SPP less aggressive for the upcoming prefetches.

Here d is the lookahead depth. 
For d = 1, when SPP is in non-speculative mode, P\textsubscript{0} can be thought of as 1. 
Once SPP enters into speculative mode, it calculates the path confidence as a product of confidence of the individual prefetches encountered along that path, further scaled using $\alpha$. 
The final P\textsubscript{d} is thresholded against prefetching-threshold (T\textsubscript{p}) to decide whether to prefetch or not. 
If yes, then P\textsubscript{d} is thresholded against a numerically bigger fill-threshold (T\textsubscript{f}) to decide whether the prefetch should be sent to L2 Cache (high confidence prefetch) or Last Level Cache (low confidence prefetch)
\newline<Insert a comprehensive figure about SPP structures>
\newline<Insert another detailed picture about SPP data-path flow>

\subsection{Case for an On-line Throttler}
As compared to some of the other state of the art prefetchers [BOP], SPP is a far less aggressive prefetcher. 
In the single core environment, this gives BOP an edge as there is no resource contention among the different cores. 
Hence more aggressive prefetching is bound to prove beneficial. 
As we increase the core count, we observe that SPP starts outperforming rest of the prefetchers. 
This can be attributed to the fact that each prefetch suggested by SPP is a carefully calculated one and that prevents cache pollution. <Figure showing the variation of SPP vs BOP wrt core count>. 
For 4-core applications, SPP suggests XX\% fewer prefetches than BOP and yet leads to higher IPC.

The above analysis shows that with a more careful throttling mechanism, SPP can be tuned to become much more aggressive, leading to increased coverage. 
The onus of maintaining the accuracy now falls on the independent throttler. 
To test the hypothesis, we tuned down SPP to the minimum possible threshold. 
Only by doing this we managed to increase the SPP coverage by XX\%. 
Obviously this came at a cost of increased DRAM traffic and cache pollution. 

Moreover, the on-line confidence mechanism introduced in SPP was very rudimentary.
It was based on taking a ratio C\textsubscript{d} = C\textsubscript{delta} / C\textsubscript{sig} as explained above.
The same confidence was used to make the decision whether to prefetch or not to prefetch; and which level to prefetch.
While this approximation was shown to work in the original SPP, we believe that a better form of on-line decision making was possible.
Hence, it was necessary to build a robust and adaptable learning mechanism to accept / reject the prefetch suggestions; and to decide the fill level (L2 Cache vs Last Level Cache).

To that effect, we introduce an independent on-line perceptron based throttling mechanism.

\section{SPP-Perc}

\subsection{Changes made to original SPP}
To modify the SPP Design to suit our scheme, the following changes were made:

\begin{itemize}
\item \textbf{Original Thresholds discarded}\newline
In the original SPP design, an internal confidence mechanism was developed which was compared against a preset PF\_THRESHOLD (T\textsubscript{p}) to decide whether to accept or reject the prefetch suggestion.
Furthermore, the accepted suggestions were thresholded against FILL \_THRESHOLD (T\textsubscript{f}) to decide the fill level: L2 Cache vs Last Level Cache.
T\textsubscript{p} and T\textsubscript{f} were set to 25 and 90 respectively, on the scale of 0 to 100.
In SPP-Perc, the perceptron sum is used to make these decisions. 
Hence these two thresholds are no longer needed

\item \textbf{Looking at L2 MSHR Queue while prefetching}\newline
Original SPP would not consider the slots available in the L2 MSHR queue. 
It had an assumption that at no time can the number of prefetches suggested by SPP could exceed the capacity of the queue. 
While it was not proven in the paper, the internal confidence mechanism along with the thresholds made sure that the above assumption was maintained. 
In SPP-Perc since there can be no assumption on the degree of aggressiveness - because SPP is being tuned to its lowest thresholds, we had to explicitly make sure that at no point do the number of suggested prefetches exceed L2 MSHR queue.

\item \textbf{Enhanced Filter}\newline
Original SPP introduces a concept of Prefetch Filter, which keeps a track of up to 1024 L2 Cache prefetches suggested by SPP. 
The Filter was modified to store meta-data like program counter, memory access address etc. 
This is needed to train the perceptron when the result of that particular prefetch gets available. 
At training time, this data is used to retrieve the state of the prefetcher when the prediction was made.

\item \textbf{Reject Filter introduced}\newline
The Prefetch Filter mentioned above keeps a track of the prefetches that actually happened in the L2 Cache. 
Correspondingly, we introduced a reject filter. 
It keeps a track of the prefetches that were suggested by the base SPP engine but were rejected by the perceptron. 
In case it is detected that a future demand fetch could have used this rejected prefetch, the perceptron weights are updated to reflect the same

\end{itemize}

\subsection{The Perceptron Layer}
As discussed earlier, the perceptron layer was introduced to act as a throttler to the underlying SPP engine. 
This section explores the details of the perceptron layer and the range of features that are ultimately used by the perceptron for correlating the prefetches.

The neural network weights are stored in separate tables for each feature.
The indexing of each feature is done using different number of bits.
At most 12 bits of a feature are used to index into the perceptron table.
Certain features require more resolution power \textit{i.e.}, full 12 bits of indexing.
On the other hand, some features requiring lower resolution required as low as 7 bits of indexing.
The variable indexing was determined by studying the features and fine-tuned empirically so as to achieve a good accuracy vs area trade-off.
Exact details are mentioned under `Area Overheads' in Section XX.

A single entry in the table corresponds to a perceptron weight.
Each weight is a 5 bit counter - saturating at -16 and +15. 
Each feature gets its own dedicated table. 
Our proposed design uses 14 features. 
As the program execution begins, all the weights are initialized to 0.
\newline \newline
\textbf{INFERENCING}\newline
In the Champsim simulator, the prefetching method is triggered on every L2 Cache demand access. 
At that point, the prefetcher has the option to do or not do prefetch - in case it does, then how many cache lines to prefetch. 
The suggested prefetches can be places either in L2 Cache or in Last Level Cache. 
The prefetcher gets to decide this, and is usually done based on an internal confidence mechanism.

When SPP gets triggered, it suggests up to 4 (=Associativity of the Pattern Table) candidates for prefetch based on the demand access address. 
In each subsequent lookahead depth, SPP suggests up to 4 more prefetch candidates, as explained in section\_\_. 
All these recommended prefetches are tested in the perceptron throttler before they can access the Prefetch Filter, as in the original SPP data-path.

The perceptron looks at the micro-architectural state, or 'features', at that instant. 
Each feature indexes into the hash of up to 4096-entry table dedicated for that feature. 
Feature \#1 indexes into a particular entry in table \#1 and so on. 

Once all the weights are retrieved, they are added together, much like the traditional perceptron dot product. 
On obtaining the overall sum, it is thresholded based on the preset threshold PERC\_THRESHOLD\_LO. 
Only the prefetch candidates with the perceptron sum higher that the threshold qualify for prefetching. 
The prefetches that qualify the perceptron throttler stage go through the Prefetch Filter stage, just as in the SPP design.
The Prefetch Filter eliminates any redundant prefetch suggestions.
In SPP-Perc, all the meta-data that is required to recreate the features is also stored in the Prefetch Filter. 
At a later stage, when the feedback of the current prefetched line is available, the stored data can be used to train the perceptron. 

The above generated perceptron sum is also used to decide L2 Cache vs Last Level Cache placement of the prefetch. 
All prefetch candidates qualified till this stage are thresholded against PERC\_THRESHOLD\_HI to decide the fill level. The two thresholds: PERC\_THRESHOLD\_LO and PERC \_THRESHOLD\_HI are empirically set. 

The perceptron sum can also be seen as the sum of the individual contribution per feature.
The value of each contribution corresponds to the extent of confidence of the final output with that given feature.
By summing the individual contributions, the final perceptron sum denotes the overall confidence for that prefetch suggestion. 
In a way, by thresholding the perceptron sum on two different thresholds, we are diving the confidence scale of the prefetch suggestion into three bins.
The first bin corresponding to the lowest confidence leads to prefetch candidate being rejected.
The next bin corresponds to prefetches with a moderate confidence level. 
Such prefetches are directed to fill the bigger Last Level Cache and potentially not pollute the more scarce L2 Cache.
The highest confidence prefetches are filled in the L2 Cache.

In addition to the Prefetch Filter mentioned above, SPP-Perc also maintains a `Reject Filter'. 
Reject filter is a 1024-entry deep 1-way associative table. 
In case that a prefetch suggestion is rejected by perceptron throttler, it instead gets logged into the Reject Filter. 
The Reject Filter keeps track of 1024 entries that were suggested by the Pattern Table but got rejected by the perceptron throttler. 
This can be used to train the perceptron to avoid false negatives \textit{i.e.}, cases where prediction was to reject the prefetch but the prefetch would have been useful.

As previously stated, an enhancement to the original SPP design includes storing all the meta-data describing the program state at the instant of prefetching, in the Prefetch Filter. 
This information is useful when the perceptron needs to be updated subsequently. 
\newline \newline
\textbf{TRAINING}\newline
In the prefetching environment, feedback of a prefetch is received whenever there is an eviction or a demand access from L2 Cache. 
This acts as a trigger to update all the baseline SPP entries and to also train the perceptron throttler. 
Training of the perceptron throttler involves restoring the meta-data that was stored in Prefetch Filter or the Reject Filter. 
This is done by indexing into the corresponding filters using the cache line address of the block being used to train. (10-bits indexing + 6-bits tag matching). 
Once the state of the program at the time of prefetching is available, it is used to index into the perceptron weights table. 



If a demand access block which triggers the training  was tagged as a valid prefetch in the Prefetch Filter, it means that the earlier prefetch prediction was correct. 
In that case the perceptron weights are incremented by 1 if the predicted sum does not cross a pre-defined threshold. 
 In case it is a cache block eviction that led to training and the corresponding valid entry was found in the Prefetch Filter, it shows that the prediction made by the perceptron was wrong. 
Perceptron should have ideally rejected the prefetch suggestion as a low-confidence prefetch. 
Here the weights are decremented by 1 to reflect the same. 
In either case, weights are saturated at -16 or +15.

A secondary training mechanism also kicks in during demand fetches. 
Before the demand access triggers the next set of prefetches, it checks the Reject Filter for the valid entry. 
In case it is a hit, that means that the corresponding cache line was initially suggested by the underlying SPP engine but rejected by the perceptron throttler. 
This means that perceptron should have been more confident about that particular prefetch. 
Once such a scenario is identified, the state of the execution at the time of prefetch is retrieved from the Reject Table. 
The retrieved data is used to index into the various weights tables of the perceptron and the corresponding values are updated by +1, saturating between and -16 and +15. 
This update reflects increased confidence for the prediction corresponding to that prefetch.  

\subsection{Features used by Perceptron}

Here we discuss the various features which help correlate the prefetching decision with the program behaviour
\begin{itemize}
\item \textbf{Base address} of the demand access which triggered the prefetch. 
Since prefetch especially tend to be triggered from the L2 demand access, they tend to be correlated with the triggering address.
\item \textbf{Cache line}: It is derived from the base address as base\_addr >> LOG2\_BLOCK\_SIZE
\item \textbf{Page address}: Calculated as base\_addr >> LOG2\_PAGE\_ SIZE
\item \textbf{Current Signature}: The 12-bit signature used by SPP to index into the Pattern Table
\item \textbf{Confidence}: The integer confidence on the scale of 0 to 100 which was used in the original SPP design
\item \textbf{Delta}: The signed version of the difference in the block address that triggers the prefetch and the block address of the data being prefetched
\item \textbf{PC}: Program Counter of the Load instruction that triggers that particular prefetch
\item \textbf{PC XOR Delta}: This helps resolve a PC into a different value for each lookahead depth of SPP speculation, giving a more accurate correlation in lookahead cases
\item \textbf{PC\textsubscript{1} XOR PC\textsubscript{2} XOR PC\textsubscript{3}}: This tells SPP-Perc about the path that led to the current demand access and helps capture and branching information of the current basic block
\item \textbf{Base address XOR Delta}: If any particular value(s) of delta are highly favoured by a given address which triggered the prefetch, this composite feature can capture that information
\item \textbf{Cache line XOR Delta}: This feature captures if any particular delta is highly favoured by a given cache line address
\item \textbf{Confidence XOR Delta}: This gives a combined insight into that fact that certain delta-confidence pairs predicted by SPP might correlate more with the prefetch outcome
\item \textbf{Program Counter XOR Delta}: This tells us if a given PC favours particular value(s) of delta
\item \textbf{Current Signature XOR Delta}: 
\end{itemize} 

Some of the composite features are derive from simple hashing (XOR) of two primary features.
There is always a question of usefulness of such composite features and the `fresh' information added.
We try to justify the choice of each feature by quantifying the contribution made towards predicting prefetch behaviour, in Section XX.
Finally, as noted above, each feature indexes into its independent entry of perceptron weights.


\section{METHODOLOGY}
\subsection{Performance Model}
For testing and comparing SPP-Perc, we use Champsim[cite] simulator. 
Champsim is an enhanced version of the framework that was used for the 2nd Data Prefetch Championship[cite]. 
We model model a 1-core / 4-core / 8-core Out of Order machine. The details of the configuration parameters is summarized in Table  \ref{tab:Sim_params}.

\begin{table}[]
    \centering
    \begin{tabular}{|l|p{3.6cm}|c|}
    \hline
    Level & Configuration & Access Latency \\
    \hline
         L1 Cache & Separate I-cache and D-cache, 32 KB, 8-way & 4 cycles\\
         L2 Cache & Private, 256 KB & 8 cycles\\
         LLC & Shared, 2MB / core, 16-way & 20 cycles\\
         DRAM & 4 GB Single Channel for single-core, 8 GB Double Channel for multi-core & N?? cycles\\
    \hline
    \end{tabular}
    \caption{Memory Model Parameters}
    \label{tab:Sim_params}
\end{table}

The block size was fixed 64KB. Prefetching was only triggered at L2 Cache demand accesses but could be sent to L2 Cache or Last Level Cache. 
No L1 Data level prefetching was done. 
LRU replacement policy was used on all levels of cache hierarchies. 
Branch prediction was done using the perceptron branch predictor [cite]. 
Page size was kept as 4KB. 
Champsim operates all the prefetchers strictly in the physical address space.

\subsection{Testing Under Additional Memory Constraints}
The default single-core configuration simulates a 2MB LLC and a single channel DRAM with 12.8GB/s bandwidth. 
We extend the simulations to include memory constraints introduced in DPC-2. 
Specifically we look at the following two variations:
\begin{itemize}
    \item \textit{Low Bandwidth DRAM}: Here the DRAM bandwidth is limited to 3.2 GB/s
    \item \textit{Small LLC}: In this scenario, LLC size is reduced to 512 KB
\end{itemize}
All the multi-core simulations are only done in the default configuration.

\subsection{Workloads}
This is the first time that SPEC 2017 benchmark suite[cite] has been used to characterize and measure the prefetch performance. 
We use all the 20 workloads available in the SPEC 2017 Suite. 
Using SimPoint[cite] methodology, we identified 95 different program segments of 1 Billion instructions each.

We performed cross-validation of our SPP-Perc model using SPEC 2006 and CloudSuite benchmarks.
For SPEC 2006, we developed 94 simpoints spread across the 29 applications.
For CloudSuite, we used the traces made available for the 2nd Cache Replacement Competition (CRC-2) [cite].
The traces include 4 multi-core applications with 6 distinct phases per application. 
In total, we used 285 traces representing workloads across 53 applications. 

\textit{Single-core performance} For single-core simulations, we use 200 Million instructions to warm-up the microarchitectural structures and next 1 Billion instructions to do detailed simulations and collect run-time statistics. 
We report the performance as speedup over baseline \textit{i.e.}, no prefetching. 
We take the Instruction Per Cycle (IPC) of the prefetcher and divide it with the IPC of baseline. 
The final number reported is the geometric mean of the speed-up attained on individual traces.

\textit{Multi-core performance} For multi-application workloads, we generate 100 completely random mixes and another 100 mixes from the memory intensive subset of SPEC 2017.
For 4-core workload, 200 Million instructions are used for warm-up and additional 1 Billion instruction simulated for collecting statistics. 
Each CPU keeps executing its workload till the last CPU completes 1 Billion instructions after warm-up. 
For collecting IPC and other data, only the first billion instructions are considered as the ``region of interest''. 

Here we report the weighted speedup normalized to baseline \textit{i.e.}, no prefetching. 
For each of the workloads running on a particular core of the 4-core 8 MB LLC system, we compute IPC\textsubscript{i}. 
We then find the IPC\_isolated\textsubscript{i} of the same workload running in isolated 1-core 8 MB LLC environment. 
Then we calculate the total weighted-IPC for a given workload mix as $\Sigma$ (IPC\textsubscript{i} / IPC\_isolated\textsubscript{i}). 
For each of the 100 workload-mix, the sum obtained is normalized to the weighted-IPC calculated similarly for baseline case \textit{i.e.}, no prefetching, to get the weighted-IPC-speedup. 
Finally the geometric mean of these 100 weighted-IPC-speedup is reported as the effective speedup obtained by the prefetching scheme.

We repeat the same process for 8-core workloads with the exception that 20 Million warm-up instructions and 100 Million full instructions were executed.
This was done so as to keep the simulation run-time within reasonable limits as single 8-core mix was taking up to 5 days of execution time.

\subsection{Preferchers Simulated}
We compared SPP-Perc against three of the latest state of the art prefetchers: Best Offset Prefetcher (BOP)[cite], DRAM Aware - Access Map Pattern Matching (DA-AMPM) [cite] and Signature Path Prefecher (SPP)[cite]. 
BOP was the winner in 2nd Data Prefetching Championship[cite]. 
DA-AMPM is the enhanced version of AMPM[cite], modified to account for DRAM row buffer locality. 
SPP has been shown to outperform BOP on SPEC 2006 traces[cite]. 
For each of these, we compare their speedups taking no prefetching as the baseline.


\subsection{Developing Features for SPP-Perc}
This section describes the intuition and analysis that went behind finalizing the features. 
As noted earlier, we have developed a set of 14 features which allow the perceptron throttler to correlate prefetching decision with the program context. 
To study the correlation across each feature, we study statistically the perceptron weights and try to interpret their distribution.
\newline  \newline
\textbf{GLOBAL PEARSON'S CORRELATION}\newline
For this experiment, we create a dump of the state of perceptron weights at the end of all trace execution.
The weights obtained from running all the SPEC 2017 traces are concatenated together. 
Since the weights are collected at the end of individual trace execution, the perceptron weights have attained a relatively stable value by now.
Hence, this dump represents a `snap' of the trained perceptron weights across all the SPEC 2017 traces.
The intuition here is that the feature with bulk of the perceptron weights concentrated around 0 or small magnitude numbers show a weak correlation with the prefetching outcome. 
On the other hand, features with most of the weights saturated around highest value (+15) show a high positive correlation and the features with weights close to the lowest value (-16) show a strong negative correlation.

We plot a histogram for each feature depicting weights distribution from -16 to +15 and use this histogram to generate the Pearson's correlation factor for that feature.
In statistics, Pearson's factor is a numerical measure of the degree of linear correlation between two variables.
It ranges from -1 to +1.
The magnitude of Pearson's factor depicts the extent of correlation and the sign depicts whether it is a positive correlation or a negative correlation.
Values close to 0 suggest a low correlation or at times, noisy data.
A value of +1/-1 suggests a perfectly linear positive / negative correlation respectively.
Figure XX depicts the histogram and the Pearson's coefficient for two of the features, YY and ZZ.

Figure XX shows all the features used sorted in increasing order of their Pearson's factor.
As can be seen X out of 14 features provide a moderate to high correlation, with the magnitude of P-value > 0.4.
The single most important feature, \_\_ helps provide a correlation to prefetch outcome with a factor of \_\_.
\newline \newline
\textbf{PER TRACE CORRELATION} \newline
Another important way to look at the perceptron features is to see how much their contribution varies across the traces.
Here we give special attention to features with low P-values in the previous experiment.
Figure XX shows the variation of P-values three features : \_\_, \_\_ and \_\_; across all the SPEC 2017 traces.
For simplicity, the traces are arranged in an increasing order of contribution made by the feature and the trace names have been omitted.
It can be seen that even features with a low overall correlation provide useful correlation (magnitude > 0.4) for XX out of the 95 simpoints developed for SPEC 2017.
\newline \newline
\textbf{USING PEARSON'S CORRELATION} \newline
Besides providing interesting insights into prefetching behaviour, P-value can also be used for feature selection and prefetcher tuning.

Here, we introduce the concept of cross-correlation across features.
Just as we examined correlation of each feature with the final outcome, we can also study correlation between the features.
We used the above methodology to eliminate features which provide a little or no information that has already been captured in other features.
As a part of feature selection, we initially came up a mix of 23 features- primary and composite.
By studying cross correlation of each of these features against others in a 23x23 matrix, we identified pairs of features with correlation factor > 0.95 in magnitude.
Using this approach, we managed to reduce the feature count to 14.
Thus, in the final implementation of SPP-Perc, no two features have a high correlation between them

[TODO: VALIDATE THIS] Secondly, studying the relative importance of each feature enabled us to vary the number of entries dedicated for each feature.
Features with higher Pearson's correlation (like \_\_ and \_\_) were given most importance and allowed full 12-bits of indexing.
Features like \_\_ and \_\_, with a low overall P-value were allocated fewer indexes in the feature table.

To conclude, in the above discussion, we justify the features for perceptron from a statistical viewpoint.
We also show how this information can also be used for prefetcher tuning.
All this study was made possible only because we used on-line perceptron learning for prefetching and that enabled us to examine the weights in detail.


\subsection{Overhead for SPP-Perc}

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|m{4.8cm}|}
    \hline
        \textbf{Field} &
        \textbf{Bits} &
        \textbf{Comment} \\
    \hline
         Valid & 1 & Indicates a valid entry in the filter\\
         Tag & 6 & Identifier for the entry in the filter\\
         Useful & 1 & To show if the given entry led to a useful demand fetch\\
         Perc Decision & 1 & Prefetched vs Not-prefetched \\
    \hline
        PC & 12 & \multirow{5}{4.8cm}{Meta-data required for perceptron training}\\
        Address & 24 & \\
        Delta & 7 & \\
        Curr Signature & 12 & \\
        Confidence & 7 & \\
    \hline
        \multicolumn{3}{|c|}{\textbf{Total} 84 bits}\\
    \hline
    \end{tabular}
    \caption{Meta-data Stored in PF and RF}
    \label{tab:PF_metadata}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{|p{1.5cm}|p{1cm}|c|p{1.5cm}|}
    \hline
        \textbf{Structure} &
        \textbf{Entry} &
        \textbf{Components} &
        \textbf{Total} \\
    \hline
        \multirow{5}{1.5cm}{Signature Table} & \multirow{5}{1.5cm}{256} & Valid (1 bit) & \multirow{5}{1cm}{11008  bits}\\
        & & Tag (16 bits) &\\
        & & Last Offset (6 bits) &\\  
        & & Signature (12 bits) &\\
        & & LRU (6 bits) &\\
    \hline
        \multirow{3}{1.5cm}{Pattern Table} & \multirow{3}{1.5cm}{512} & $C_{sig}$ (4bits) &\multirow{3}{1cm}{24576 bits}\\
        & & $C_{delta}$ (4*4 bits) &\\
        & & Delta (4*7 bits) &\\
    \hline
        \multirow{4}{1.5cm}{Perceptron Tables} & 4096*7 & \multirow{4}{1.5cm}{5 bits} & \multirow{4}{1.5cm}{186880 bits}\\
        & 2048*4 & &\\
        & 256*1 & &\\
        & 128*2 & &\\
    \hline
        \multirow{2}{1.5cm}{Prefetch Filter \footnotemark[1]} & \multirow{2}{1.5cm}{1024} & \multirow{2}{1.5cm}{84 bits} & \multirow{2}{1.5cm}{86016 bits}\\
        & & &\\
    \hline
     \multirow{2}{1.5cm}{Reject Filter \footnotemark[2]} & \multirow{2}{1.5cm}{1024} & \multirow{2}{1.5cm}{83 bits} & \multirow{2}{1.5cm}{84992 bits}\\
        & & &\\
    \hline
        \multirow{4}{1.5cm}{Global History Register} & \multirow{4}{1.5cm}{8} & Signature (12 bits) & \multirow{4}{1.5cm}{264 bits}\\
        & & Confidence (8 bits) & \\
        & & Last Offset (6 bits) & \\
        & & Delta (7 bits) & \\
    \hline
        \multirow{2}{1.5cm}{Accuracy Counters} & 1 & C$_{total}$ & 10 bits\\
        & 1 & C$_{useful}$ & 10 bits\\
    \hline
        \multirow{3}{1.5cm}{Global PC Trackers} & \multirow{3}{1cm}{3} & $PC_1$ (12 bits)& \multirow{3}{1.cm}{36 bits}\\
        & & $PC_2$ (12 bits) &\\
        & & $PC_3$ (12 bits) &\\
    \hline
        \multicolumn{4}{|c|}{\textbf{Total: 393792 bits = 48.07 KB}}\\
    \hline
    \end{tabular}
    \caption{SPP-Perc Storage Overhead}
    \label{tab:SPPPerc_overhead}
\end{table}
\footnotetext[1]{Components of Prefetch Filter can be found in Table \ref{tab:PF_metadata}}
\footnotetext[2]{RF does not need to maintain the useful bit as that only applies for prefetches that ultimately made through}

In this section, we analyze the hardware overhead required to implement SPP-Perc. 
The Prefetch Filter was enhanced to accommodate storing of meta-data for perceptron training. 
Table \ref{tab:PF_metadata} depicts the meta-data stored for each entry in the Prefetch Filter.
Table \ref{tab:SPPPerc_overhead} shows the total storage overhead of SPP-Perc implementation.
The hardware budget for 2nd Data Prefetching championship was 32 KB. 
Keeping that in mind and the speedup SPP-Perc obtained over the winner, the extra hardware budget can be accounted for.
The extra hardware also makes the overall scheme more scalable than SPP. 
In the original SPP paper, it was demonstrated that adding extra hardware brings little advantage in terms of performance gain.
The newly added perceptron tables can be scaled to increase / decrease features depending on the permitted budget.

In terms of computations, the perceptron mechanism only introduces an extra adder tree.
The hash perceptron mechanism makes sure than there is no actual vector multiplication happening in the hardware.
Obtaining the perceptron sum requires addition of 14 5-bit numbers. 
Using an adder tree of 7 5-bit adders, this can be done in ceil($log_214$) = 4 steps.
Perceptron update only requires weight update by +1 or -1.
Thus, all the operations required for perceptron inferencing or updating the states of the perceptron throttler can be easily done in the time constraints of L2 Cache Accesses.


\section{RESULTS}
This section discusses the results obtained from running SPP-Perc, in terms of cache Misses Per Kilo Instructions (MPKI) and Instruction Per Cycle (IPC) metrics. 
For SPEC 2017 suite, first we present the results for single-threaded workloads then the results for multi-core workloads are presented.

\subsection{Single-core Results}
Figure \_\_ shows the IPC speedup of the 4 prefetchers simulated. 
All the results have been normalized to baseline \textit{i.e.}, no prefetching. 
As can be seen, SPP-Perc yields a speedup of XX\% over the baseline. 
This is equivalent to XX\% over DA-AMPM, xx\% over BOP and XX\% over SPP. 
Out of the XX benchmarks, SPP-Perc nearly matches or outperforms most of the prefetchers on XX traces. 

At its peak, SPP-Perc manages to get the IPC speedup of a factor of over \textbf{3.4} on the trace \textit{602.gcc\_s-2226B}. 
This also corresponds to speedup gain of \textbf{22.4\%} over the next best prefetcher - SPP. 
In general, benchmarks \textit{602.gcc\_s, 603.bwaves\_s, 605.mcf\_s, 621.wrf\_s} and \textit{649.fotonik3d\_s} benefit the most from SPP-Perc, as compared to SPP only prefetching.

<Discuss the extreme behaviour of \textit{623.xalancbmk\_s}: Improvement over SPP is as high as 31\% and even reduces to -1.5\% on traces of this benchmark>

<Discuss about coverage on all traces>

<Discuss cases where coverage gets specially increased>

<Discuss accuracy on all traces>

<Discuss about compelling increase in accuracy - details of traces. Plot linear separability>

<Any case where accuracy got reduced - go into the trace. See if linear separability is the issue. Other possible issues?>


\subsection{Multi-core Results}
In this section, we demonstrate the improvement achieved by SPP-Perc for a mix of multi-programmed workloads. 
\newline \newline
\textit{4-core environment}
Figure \_\_ shows a comparison of speedups obtained on workload mix of memory intensive subset of SPEC 2017. 
We plot all the 4 prefetchers, normalized to baseline. 
The workloads have been sorted in the increasing order of performance benefit, as measured in terms of normalized weighted speedup. 
As can be seen, SPP-Perc offers an IPC improvement by 33.5\% on these traces. 
This is 5.5\% over the next best prefetcher SPP; and XX\% and YY\% over BOP and DA-AMPM respectively.

This increased improvement by SPP-Perc on a 4-core machine is not surprising. 
Since SPP-Perc has an extremely intelligent throttler working to eliminate useless prefetches before they can create cache pollution. 
This further amplifies the edge that SPP had over BOP. 
With the advantage of an orthogonal prefetcher, SPP can be as aggressive as possible and leave the accuracy aspect to perceptron throttler.
\newline \newline
\textit{8-core environment}
To test the effectiveness of SPP-Perc in a tighter constrained multi-core environment, we simulated an 8-core machine with 16 MB last Level Cache. 
Other parameters were kept same as discussed under Methodology in Section XX.

\subsection{Additional Memory Constraints}
Figure \_\_ shows the performance of SPP-Perc against the other prefetchers in tighter constraint environments. 
For sake of simplicity, we show 2 traces where SPP-Perc gives the most advantage over baseline, 2 traces where SPP-Perc doesn't perform as well as the best prefetcher *<-replace with the prefetcher name once results are available* and finally GeoMean across all the traces. 
<Hopefully BOP would degrade more here as it is super aggressive. 
By definition, SPP-Perc should be better than SPP as the throttler would better control the aggressiveness. 
In SPP paper DA-AMPM performs close enough to BOP. That should get replicated here too.>

\subsection{Cross Validation}


\section{CONCLUSION}
In this paper, we introduced perceptron based learning for data prefetching in the form of SPP-Perc. 
Perceptron acts an orthogonal throttler to the underlying prefetch engine. 
SPP-Perc improves performance by over XX\% over the baseline, which corresponds to YY\% over the next best performing prefetcher. 
We believe this paper can lead to interesting future work like generalizing SPP-Perc to work with any prefetcher with minimal knowledge of the underlying prefetcher architecture.

%%%%%%% -- PAPER CONTENT ENDS -- %%%%%%%%

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
%%\bibliographystyle{ieeetr}
%%\bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
