===========================================================================
THE REBUTTAL DRAFT
===========================================================================

We thank the reviewers for their thoughtful comments.

*Performance Benefits*
(#34F) PPF performs better in 4/8-core systems due to better shared
resource (LLC) utilization.  With more apps sharing the LLC, the costs
of pollution increase, thus better filtering improves performance
more.

*Overheads*
(#34A) An additional 39KB of state (7KB more than most) leads to
significant improvement over state-of-the-art prefetchers, especially
for the multi-core case. The logic overhead is minimal.

*Timeliness*
(#34A) SPP's confidence and PPF's perceptron sum measure usefulness,
which is a combination of accuracy and timeliness. Untimely prefetches
lead to cache misses, rendering the prefetch useless in terms of
prefetcher learning.  Maximizing the probability of usefulness leads
to increased timeliness.

*Cache Pollution*
(#34C) Overly aggressive (hence less accurate) prefetching leads to
both cache pollution and excessive bandwidth utilization, ultimately
they both impact performance, as shown in Sec1/Fig1.

(#34F) Inferring and segregating prefetch-induced cache misses, other
than their impact on coverage, is a difficult problem.  Doing so
likely requires getting a full trace of memory misses in each case
and comparing traces.  Unfortunately that is infeasible, so we
typically rely upon coverage metrics and how prefetchers affect
overall performance, especially in multi-core systems.

*Multiple Prefetches*
(#34F) On a demand access, the prefetcher produces only as many
prefetches as the L2C MSHR can accommodate. Any pending prefetches,
i.e., produced but not issued, are dropped by the cache controller
after a new demand access, giving up speculation in favor of ground
truth.

*L1 Prefetcher*
(#34B) Nothing prevents implementing SPP+PPF as an L1D Prefetcher.
However, having a 39KB overhead for a 32KB might seem extravagant.

*Training Sensitivity*
(#34B) It has been demonstrated that perceptron training is faster
than gshare-style training[Jimenez,HPCA2001]. Additionally, we can
bias the weights, offline, towards accept/reject tendency for a given
application.

*Prior Work*
(#34A) References [4] and [5] target different spaces --
compiler-driven[4] and offline parametric search for datacenter
applications[5]. We will investigate comparing with Coordinated
Control[Ebrahimi,MICRO2009].

*Paper Edits*
We will:
- (#34C) cite "Learning Memory Access Patterns"
- (#34E) quantify the effect of PPF on look-ahead values
- (#34F) rectify confusion regarding baseline prefetcher and baseline
         of no-prefetching
- (#34F) show the reject table in Fig5

*Future Work*
(#34D) Using PPF to generate prefetches is an excellent idea that is
worth looking into.
(#34D) Using PPF to give feedback to SPP, this is ongoing work.
(#34B and #34E) Multi-threaded workloads would need to a mechanism to
manage invalidations and find a way to speculate beyond
synchronization points, we will examine this in future work.

*Clarifications*
(#34A) Combined features convey some information not captured
individually (Sec4.2).

(#34A) Overall LLC Latency: 4+8+12=24 (considering L1/L2 miss
penalty).  DRAM Latency: 100-200 cycles, depending on contention in
the detailed DRAM simulator.

(#34D) SPP's confidence predictor hasn't been ignored; as described in
Sec2.1 it is used as a pertinent feature for PPF.

(#34D) Performance of mcf: SPP uses SPEC 2006 while we use SPEC 2017,
though the same original application the code has evolved and the
input set is different.

