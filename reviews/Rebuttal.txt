===========================================================================
THE REBUTTAL DRAFT
===========================================================================

*Performance Benefits*
The prefetcher gives better benefits in 4/8-core systems primarily due to effective 
shared resource (LLC) utilization. Each core sees a bigger total LLC, so more 
potential for an aggressive prefetching to hog the useful resources of another 
core. Effective filtering can help avoid that. (#34F)

*Overheads*
An additional 39KB of hardware (7KB more than most) leads to a significant improvement 
over state-of-the-art prefetchers, especially in the multi-core case. There are no
significant logic overheads. (#34A)

*Timeliness*
We think of SPP’s confidence and PPF’s perceptron sum as a measure of usefulness, 
which is a combination of accuracy and timeliness. An untimely prefetch will lead to 
a cache miss, which renders the prefetch useless, in term of prefetcher learning. 
Maximizing the probability of usefulness leads to increased timeliness (#34A)

*Cache Pollution*
Overly aggressive (hence less accurate) prefetching leads to cache pollution and 
a decline of system performance, as shown in Sec1/Fig1 (#34C)
It is an interesting and equally difficult problem to infer and segregate 
cache misses introduced by prefetches, other than reduced coverage. Same for 
prefetches which evict an otherwise useful block. These behaviors are reflected in 
overall performance, especially in multi-core systems. (#34F)

*Scheduling Prefetches*
On a demand-fetch, the prefetcher recommends only as many prefetches as the L2C MSHR 
can accommodate. Any pending prefetches i.e., recommended but not issued, get dropped 
by the cache controller if a new demand fetch is seen. Basically, giving up speculation 
in favor of ground truth. (#34F)

*L1 Prefetcher*
Nothing prevents implementing SPP+PPF as an L1D Prefetcher. However, having a 39KB 
overhead for an 32KB cache is impractical and combinational logic can’t be implemented 
within L1 latency frame (#34B)

*Training Sensitivity*
It has been demonstrated in "Dynamic Branch Prediction with Perceptrons[Jimenez,HPCA-’01]" 
that perceptron training is faster than Gshare style training. Additionally, we can 
bias the weights, offline, towards accept/reject tendency for a given application. (#34B)

*Prior Work*
Comparison with [4] and [5] doesn’t make sense as they target different space -- compiler-driven [4] 
and offline parametric search for datacenter applications[5]. We will investigate comparing with 
Coordinated Control [Ebrahimi, MICRO-’09] (#34A)

*Paper Edits*
Cite "Learning Memory Access Patterns" (#34C)
Effect of PPF on look-ahead values -- increased by __% (#34E)
Rectify confusion regarding baseline prefetcher and baseline of no-prefetching. 
Showing the reject table in Fig5 (#34F)

*Future Work*
Using PPF to generate prefetches is an excellent idea that is worth looking into.
Using PPF to give feedback to SPP (#34D)
Multi-threaded version of PPF (#34B and #34E)

*Clarifications*
Combined features convey some information not captured individually (Sec4.2). 
Overall LLC Latency: 4+8+12=24 (Considering L1/L2 miss penalty)
DRAM Latency: 100-200 cycles, depending on contention. (#34A)
SPP’s confidence predictor hasn't been completely ignored. 
Described in Sec2.1 and has been used as a pertinent feature for PPF. 
Probably because the SPP paper uses SPEC 2006 while we use SPEC 2017. mcf uses different 
input sets for these two -- combinatorial optimization vs route planning (#34D)