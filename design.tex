\section{PPF Architecture}
\label{Design}

In this section we describe our implementation of PPF.  PPF is a generalised
prefetch throttler and can be adapted to most prefetchers with minimal
modifications.  For a practical implementation, we have used SPP as the base
engine prefetcher.

\subsection{Changes made to original SPP}
\label{Design-Changes}
To modify the SPP design to suit our scheme, the following changes were made:

\begin{itemize}
\item \textbf{Original Thresholds discarded}\newline In the original
  SPP design, the internal confidence was compared against a preset
  PF\_THRESHOLD (T\textsubscript{p}) to decide whether to accept or reject the
  prefetch suggestion.  Furthermore, the accepted suggestions were thresholded
  against FILL \_THRESHOLD (T\textsubscript{f}) to decide the fill level: L2
  Cache vs Last Level Cache.  T\textsubscript{p} and T\textsubscript{f} were
  set to 25 and 90 respectively, on the scale of 0 to 100.  In PPF, the
  perceptron sum is used to make these decisions.  Hence these two thresholds
  are no longer needed

%%[NOTE] Can this point be removed?
% djimenez: I think this would be necessary to reproduce our results so leave it in. 

\item \textbf{Looking at L2 MSHR Queue while prefetching}\newline The original
SPP would not consider the slots available in the L2 MSHR queue.  It had an
assumption that at no time can the number of prefetches suggested by SPP
exceed the capacity of the queue.  While it was not proven in the paper, the
internal confidence mechanism along with the thresholds made sure that the
above assumption was maintained.  In PPF since there can be no assumption on
the degree of aggressiveness of the underlying prefetcher, we explicitly check
that at no point should the number of suggested prefetches exceed the L2 MSHR
queue.

\item \textbf{Enhanced Filter}\newline The original SPP introduces a concept of
the Prefetch Filter that keeps a track of up to 1024 L2 cache prefetch
suggestions.  For PPF, the Filter was modified to store metadata such as program
counter, memory access address etc.  These features are needed to train the perceptron
when the result of that particular prefetch becomes available.  At training time,
this data is used to retrieve the state of the prefetcher when the prediction
was made.

\item \textbf{Reject Filter introduced}\newline The Prefetch Filter mentioned
above keeps track of the prefetches that actually happened in the L2 Cache.
Correspondingly, we introduced a reject filter.  It keeps track of the
prefetches that were suggested by the base SPP engine but were rejected by the
perceptron.  In case it is detected that a future demand fetch could have used
this rejected prefetch, the perceptron weights are updated to reflect that
misprediction.

\end{itemize}

\subsection{The Perceptron Layer}
\label{Design-Perceptron}
As discussed earlier, the perceptron layer was introduced to act as a
throttler to the underlying prefetch engine.  This section explores the
details of the perceptron layer and the range of features that are ultimately
used by the perceptron for correlating the prefetches.

The weights are stored in separate tables for each
feature.  Features are indexed using different numbers
of bits.  At most 12 bits of a feature are used to index into a
weights table.  Certain features require more resolution power
\textit{i.e.}, the full 12 bits of indexing.  On the other hand, some
features requiring lower resolution require as few as 7 bits of
indexing.  The variable indexing was determined by studying the
features and fine-tuned empirically so as to achieve a good accuracy
vs area trade-off.  Exact details are mentioned under ``Area Overheads''
in Section~\ref{Method-Overheads}

A single entry in the table corresponds to a perceptron weight.  Each
weight is a 5 bit counter - saturating at -16 and +15.  Each feature
has its own dedicated table.  Our proposed design uses 9 features.
As program execution begins, all the weights are initialized to 0.

\textbf{Inference}\newline In the ChampSim simulator, the prefetching method
is triggered on every L2 Cache demand access.  At that point, the prefetcher
has the option to do or not do a prefetch -- if it does, then it has a choice
in how many cache lines to prefetch.  The suggested prefetches can be either
in the L2 Cache or the last-level cache.  The prefetcher decides this
placement, and it is usually done based on an internal confidence mechanism.

When the base prefetcher is triggered, it starts suggesting candidates for
prefetching.  All these recommended prefetches are tested in the perceptron
throttler before they can access the Prefetch Filter, as in the original SPP
datapath.

The perceptron looks at the microarchitectural state, {\em i.e.} the features,
at that instant.  Each feature is hashed to form an index into a table of up to 
4096 entries dedicated for that feature.  
Feature \#1 indexes into
a particular entry in table \#1 and so on.

Once all the weights are retrieved, they are summed.  The sum is thresholded
based on the preset threshold PERC\_THRESHOLD\_LO.  Only the prefetch
candidates with perceptron sum higher that the threshold qualify for
prefetching.  The prefetches that qualify through the perceptron throttler
stage go through the Prefetch Filter stage.  The Prefetch Filter eliminates
any redundant prefetch suggestions.  In PPF, all the metadata that is required
to recreate the features is also stored in the Prefetch Filter.  At a later
stage, when the feedback of the current prefetched line is available, the
stored data is used to train the perceptron.

The perceptron sum is also used to decide L2 Cache vs Last Level Cache
placement of the prefetch cache line.  All prefetch candidates qualified till
this stage are thresholded against PERC\_THRESHOLD\_HI to decide the fill
level. The two thresholds: PERC\_THRESHOLD\_LO and PERC \_THRESHOLD\_HI are
empirically set.

The perceptron sum can also be seen as the sum of the individual contribution
per feature.  The value of each contribution corresponds to the extent of
confidence of the final output with that given feature.  By summing the
individual contributions, the final perceptron sum denotes the overall
confidence for that prefetch suggestion.  By thresholding the perceptron sum
on two different thresholds, we are dividing the confidence scale of the
prefetch suggestion into three bins.  The first bin corresponds to the lowest
confidence leads to prefetch candidate being rejected.  The next bin
corresponds to prefetches with a moderate confidence level.  Such prefetches
are directed to fill the bigger Last Level Cache and potentially not pollute
the more scarce L2 Cache.  The highest confidence prefetches are filled in the
L2 Cache.

In addition to the Prefetch Filter mentioned above, PPF also maintains a
``Reject Filter.''  The reject filter is a 1024-entry deep direct-mapped
table.  If a prefetch suggestion is rejected by perceptron throttler, it is
logged into the Reject Filter.  The filer is used to train the perceptron to
avoid false negatives \textit{i.e.}, cases where prediction was to reject the
prefetch but the prefetch would have been useful.

% djimenez: if it were previously stated then it's redundant here? do we need
% this statement?

%As previously stated all the metadata describing the program state at the
%instant of prefetching is stored in the Prefetch Filter or the Reject Filter.
%This information is useful when the perceptron needs to be updated
%subsequently.

\textbf{Training}\newline In the prefetching environment, feedback for a
prefetch is received whenever there is an eviction or a demand access from the
L2 Cache.  This action triggers training of the base prefetcher as well as the
perceptron throttler.  Training the perceptron throttler involves restoring
the metadata that was stored in Prefetch Filter or the Reject Filter.  This is
done by indexing into the corresponding filters using the cache line address
of the block being used to train. (10-bits indexing + 6-bits tag matching).
Once the state of the program at the time of prefetching is available, it is
used to index into the perceptron weights table.

If a demand access block that triggers the training was tagged as a valid
prefetch in the Prefetch Filter, then the earlier prefetch prediction was
correct.  In that case the perceptron weights are incremented by 1 if the
predicted sum does not cross a pre-defined threshold.  If a cache block
eviction led to training and the corresponding valid entry was found in the
Prefetch Filter, then the prediction made by the perceptron was wrong.  The
perceptron should have ideally rejected the prefetch suggestion as a
low-confidence prefetch.  Here the weights are decremented by 1 to reflect the
misprediction. In either case, weights are saturated at -16 or +15.

A secondary training mechanism also kicks in during demand fetches.  Before
the demand access triggers the next set of prefetches, Reject Filter is
checked for a valid entry.  A hit means that the corresponding cache line was
initially suggested by the underlying prefetch engine but rejected by the
perceptron throttler.  Thus, the perceptron should have been more confident
about that particular prefetch.  Once such a scenario is identified, the state
of the execution at the time of prefetch is retrieved from the Reject Table.
The retrieved data is used to index into the various weights tables of the
perceptron and the corresponding values are updated by +1, saturating between
and -16 and +15.  This update reflects increased confidence for the prediction
corresponding to that prefetch.

This mechanism allows us to solve the classical problem of exploiting a lost
opportunity.  In prior perceptron based implementations (and in general,
prefetching algorithms), there is usually no way of knowing the result of not
prefetching a particular line.  Our two-step PPF architecture allows us to
overcome that issue.

\subsection{Features used by Perceptron}
\label{Design-Features}
Here we discuss the various features that correlate the prefetching decision
with the program behaviour.

\begin{itemize}
\item \textbf{Base Address} of the demand access that triggered the
  prefetch.  Since prefetches are triggered from the L2 demand access,
  they tend to be correlated with the triggering address.

\item \textbf{Cache Line} and \textbf{Page Address}: These two
  separate features are derived from the base address that triggered
  the prefetch, as follows: base\_addr >> LOG2\_BLOCK \_SIZE and
  base\_addr >> LOG2\_PAGE\_ SIZE respectively.  The idea behind using
  three different shifted versions of the same feature is that it
  allows us to look into a wider range of bits than with a single
  version.  It also helps give more importance to the overlapping bits
  and lesser importance to most and least significance bits.  This
  approach can also eliminate destructive interference that can be
  caused by directly folding the bits into half.

% djimenez: this is a nice intuition and explanation.

\item \textbf{Program Counter XOR Depth}: The PC is for the instruction
  that triggered the prefetch chain.  Depth refers to the iteration count of
  the lookahead stages.  In general, the PC is not considered as a good basis
  for doing lookahead prefetching as all the prefetches with depth >= 1 are
  aliased into the same PC which would not be true in an actual demand access.
  This feature resolves a PC into a different value for each lookahead depth
  of prefetch speculation, giving a more accurate correlation in lookahead
  cases.  This is akin to the concept of Virtual Program Counters~\cite{VPC}
  introduced by Kim \textit{et. al.} for indirect branch prediction.

\item \textbf{PC\textsubscript{1} XOR PC\textsubscript{2}>>1 XOR
  PC\textsubscript{3}>>2}: Here $PC_i$ refers to the last $i^{th}$
  PC before the instruction that triggered the current prefetch.
  Hashing together the last three PCs tell PPF about the path
  that led to the current demand access and helps capture and
  branching information of the current basic block.  PCs are shifted
  in the increasing order of history before being hashed together.
  This is done to avoid the resultant value of zero in case 2 or more
  PCs are the same.  Additionally, obfuscating the information as it
  gets old allows us to get a wider and an approximate look into the
  program history.

\item \textbf{Program Counter XOR Delta}: This feature tells us if a given PC
  favours particular value(s) of delta.  As noted earlier, while the PC in
  itself does not convey much useful information, this hash resolves the PC
  into different values based on the tendency of that PC to favour a certain
  delta.  Thus, the dynamic nature of different instances of the same memory
  access instruction can be captured here.

\item \textbf{Confidence}: The integer confidence on a scale of 0 to
  100 that was used in the original SPP design.  While the original confidence
  is not used directly for decision making, it can still contribute to the
  final decision made.

% djimenez: this is a little vague. can you be more precise?
\item \textbf{Current Signature XOR Delta}: Recall from
  the discussion of SPP in Section \ref{Background-SPP} that the new signature
  is generated using the old signature and the block delta.  While ``Current
  Signature XOR Delta'' is not the exact formula for generating the future
  signature, it gives an approximate idea of the path that the combination of
  these two values can lead to.

\item \textbf{Page Address XOR Confidence}: This feature scores the tendency
of each page to be prefetch friendly or prefetch averse. It helps resolve a
page into different entries depending on its confidence for prefetching, which
can vary during phases of a program execution.

%%%%%%% Below are the features rejected when downsizing 14 -> 9
% \item \textbf{Current Signature}: The 12-bit signature used by SPP
%   to index into the Pattern Table
% \item \textbf{Delta}: The signed version of the difference in the
%   block address that triggers the prefetch and the block address of
%   the data being prefetched
% \item \textbf{PC}: Program Counter of the Load instruction that
%   triggers that particular prefetch
% \item \textbf{Base address XOR Delta}: If any particular value(s) of
%   delta are highly favoured by a given address which triggered the
%   prefetch, this composite feature can capture that information
% \item \textbf{Cache line XOR Delta}: qThis feature captures if any
%   particular delta is highly favoured by a given cache line address
%\item \textbf{Confidence XOR Delta}: This gives a combined insight
%  into that fact that certain delta-confidence pairqs predicted by
%  SPP might correlate more with the prefetch outcome
%%%%%%%%%%%%%%%%%%%%%%

\end{itemize} 

Some of the composite features are derived from simple hashing (XOR) of two
primary features.  There is always a question of usefulness of such composite
features and the new information added.  We justify the choice of each feature
by quantifying the contribution made towards predicting prefetch behaviour, in
Section \ref{Method-Features}.  Finally, as noted above, each feature indexes
into its independent entry of perceptron weights.

\subsection{Generalizing PPF for any Prefetcher}
\label{Design-Generalizing}
The above implementation of PPF shows that it is highly modular and can be
adapted to be used over any base prefecher for increased prefetch accuracy.
In our implementation, only two hooks are required between PPF and the
baseline SPP. The first is to make sure that all the prefetch candidates of
SPP pass through the perceptron throttler and if qualified, the metadata for
perceptron indexing be stored. The second is needed when the feedback of a
prior prefetch is available in form of a subsequent demand hit or cache
eviction. In that case, the stored metadata needs to be retrieved to update
the state of the perceptrons.

In general, PPF can be adapted to a new base prefetcher with only a few modifications.
\begin{itemize}

\item \textbf{Enhancing the Base Prefetcher:} By tuning down any internal thresholds 
to increase its inherent aggressiveness. 

\item \textbf{Inferencing and Storing:} All prefetch recommendations to be
tested using the perceptron inferencing algorithm.  Perceptron output:
\textit{true} and \textit{false} should be saved appropriately, along with all
the metadata required for perceptron indexing.

\item \textbf{Retrieving and Training:} When the feedback is available, the
previously stored metadata can be used to re-index into the perceptron entries
and increment / decrement the weights.

\item \textbf{Feature Selection:} Six out of the nine features we developed
use information derived from program execution, irrespective of the baseline
SPP prefetcher. Beyond that, the feature set can be expanded to convey any
useful information from the prefetcher to the perceptron throttler.  For
example, in our implementation ``Confidence,'' ``Current Signature XOR Delta,''
and ``Page Address XOR Confidence'' are such features.

\end{itemize}

